{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f6784c",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79cbf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/367647213.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/367647213.py:13: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Job_name = driver.find_element_by_class_name('suggestor-input')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/367647213.py:19: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Location = driver.find_element_by_class_name('locationSugg')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/367647213.py:22: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Location.find_element_by_class_name('suggestor-input ').send_keys('Bangalore/Bengaluru')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/367647213.py:29: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/367647213.py:35: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/367647213.py:39: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  a = driver.find_elements_by_class_name('jobTupleHeader')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Company</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>No_of_Reviews</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>METRO Cash &amp; Carry</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(958 Reviews)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>15,00,000 - 27,50,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>METRO Cash &amp; Carry</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(958 Reviews)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>15,00,000 - 27,50,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data analyst/ data analytics / Business analys...</td>\n",
       "      <td>Leading US MNC into Analytics</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>10,00,000 - 20,00,000 PA.</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Delhi /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business analyst + data Analysis</td>\n",
       "      <td>Anlage Infotech (I) Pvt. Ltd.</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>upGrad</td>\n",
       "      <td>3.1</td>\n",
       "      <td>(338 Reviews)</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Associate Professional Data Analyst</td>\n",
       "      <td>DXC Technology</td>\n",
       "      <td>3.8</td>\n",
       "      <td>(5740 Reviews)</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Business and Data Analyst</td>\n",
       "      <td>CAREERDOST ENTERPRISE</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>15,00,000 - 30,00,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(9160 Reviews)</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Thomson Reuters</td>\n",
       "      <td>4.3</td>\n",
       "      <td>(1187 Reviews)</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>referral</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job_Role  \\\n",
       "1                     Data Analyst / Business Analyst   \n",
       "2                     Data Analyst / Business Analyst   \n",
       "3   data analyst/ data analytics / Business analys...   \n",
       "4                    Business analyst + data Analysis   \n",
       "5                                        Data Analyst   \n",
       "6                 Associate Professional Data Analyst   \n",
       "7                           Business and Data Analyst   \n",
       "8                                 Senior Data Analyst   \n",
       "9                                 Senior Data Analyst   \n",
       "10                                       Data Analyst   \n",
       "\n",
       "                          Company        Ratings   No_of_Reviews Experience  \\\n",
       "1              METRO Cash & Carry            4.2   (958 Reviews)    3-8 Yrs   \n",
       "2              METRO Cash & Carry            4.2   (958 Reviews)    3-8 Yrs   \n",
       "3   Leading US MNC into Analytics  Not available   Not available    2-7 Yrs   \n",
       "4   Anlage Infotech (I) Pvt. Ltd.  Not available   Not available   5-10 Yrs   \n",
       "5                          upGrad            3.1   (338 Reviews)    1-3 Yrs   \n",
       "6                  DXC Technology            3.8  (5740 Reviews)    3-6 Yrs   \n",
       "7           CAREERDOST ENTERPRISE  Not available   Not available    0-5 Yrs   \n",
       "8                        Flipkart            4.2  (9160 Reviews)    3-7 Yrs   \n",
       "9                 Thomson Reuters            4.3  (1187 Reviews)    2-4 Yrs   \n",
       "10                       referral  Not available   Not available    2-5 Yrs   \n",
       "\n",
       "                       Salary  \\\n",
       "1   15,00,000 - 27,50,000 PA.   \n",
       "2   15,00,000 - 27,50,000 PA.   \n",
       "3   10,00,000 - 20,00,000 PA.   \n",
       "4               Not disclosed   \n",
       "5               Not disclosed   \n",
       "6               Not disclosed   \n",
       "7   15,00,000 - 30,00,000 PA.   \n",
       "8               Not disclosed   \n",
       "9               Not disclosed   \n",
       "10              Not disclosed   \n",
       "\n",
       "                                             Location  \n",
       "1                                 Bangalore/Bengaluru  \n",
       "2                                 Bangalore/Bengaluru  \n",
       "3   Gurgaon/Gurugram, Bangalore/Bengaluru, Delhi /...  \n",
       "4         Hyderabad/Secunderabad, Bangalore/Bengaluru  \n",
       "5                                 Bangalore/Bengaluru  \n",
       "6                                 Bangalore/Bengaluru  \n",
       "7                                 Bangalore/Bengaluru  \n",
       "8                                 Bangalore/Bengaluru  \n",
       "9                                 Bangalore/Bengaluru  \n",
       "10                                Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(10)\n",
    "\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(10)\n",
    "\n",
    "#Locate the search bar\n",
    "Job_name = driver.find_element_by_class_name('suggestor-input')\n",
    "\n",
    "#Enter the search text\n",
    "Job_name.send_keys('Data Analyst')\n",
    "\n",
    "#Enter the location bar\n",
    "Location = driver.find_element_by_class_name('locationSugg')\n",
    "\n",
    "#Enter the location text\n",
    "Location.find_element_by_class_name('suggestor-input ').send_keys('Bangalore/Bengaluru')\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "#click the crosswrapper\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "try:\n",
    "    driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
    "except NoSuchElementException as error:\n",
    "        pass\n",
    "time.sleep(10)\n",
    "\n",
    "#click the search button\n",
    "driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
    "time.sleep(10)\n",
    "\n",
    "#extract the product card\n",
    "a = driver.find_elements_by_class_name('jobTupleHeader')\n",
    "\n",
    "#From the extracted data we find that totally there are 7 datas but for some cases there were no ratings & no of ratings\n",
    "# a template which was created for all cases,\n",
    "c,Job_Role=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "    else:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "\n",
    "c,Company=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "    else:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "\n",
    "#In this case,the order changes from here on, so change the else condition as not available for not matching the if ondition\n",
    "c,Ratings=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Ratings.append(a[i].text.split('\\n')[2])\n",
    "    else:\n",
    "        Ratings.append('Not available')\n",
    "        \n",
    "#In this case,the order changes, so change the else condition as not available for not matching the if ondition\n",
    "c,No_of_Reviews=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        No_of_Reviews.append(a[i].text.split('\\n')[3])\n",
    "    else:\n",
    "        No_of_Reviews.append('Not available')\n",
    "\n",
    "#Change the order based on the previous cases\n",
    "c,Experience=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Experience.append(a[i].text.split('\\n')[4])\n",
    "    else:\n",
    "        Experience.append(a[i].text.split('\\n')[2])\n",
    "        \n",
    "c,Salary=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Salary.append(a[i].text.split('\\n')[5])\n",
    "    else:\n",
    "        Salary.append(a[i].text.split('\\n')[3])\n",
    "        \n",
    "c,Location=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Location.append(a[i].text.split('\\n')[6])\n",
    "    else:\n",
    "        Location.append(a[i].text.split('\\n')[4])\n",
    "        \n",
    "#import pandas for preparing dataframes\n",
    "import pandas as pd\n",
    "\n",
    "#Arrange the rows based on the category and zip it for the dataframe\n",
    "df_1 = pd.DataFrame(zip(Job_Role,Company,Ratings,No_of_Reviews,Experience,Salary,Location),columns=('Job_Role','Company','Ratings','No_of_Reviews','Experience','Salary','Location'))\n",
    "df_1.index = [x for x in range(1, len(df_1.values)+1)]\n",
    "#Since 10 results only required, we use iloc\n",
    "df_1.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7fc081",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03de763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/4281197824.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/4281197824.py:11: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Job_name = driver.find_element_by_class_name('suggestor-input')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/4281197824.py:17: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Location = driver.find_element_by_class_name('locationSugg')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/4281197824.py:20: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Location.find_element_by_class_name('suggestor-input ').send_keys('Bangalore')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/4281197824.py:23: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@title=\"Bangalore/Bengaluru\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/4281197824.py:28: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/4281197824.py:33: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/4281197824.py:36: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  a = driver.find_elements_by_class_name('jobTupleHeader')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Company</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>No_of_Reviews</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>UPL</td>\n",
       "      <td>4.3</td>\n",
       "      <td>(1198 Reviews)</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sr. Analyst - Applied Data Scientist</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>4.1</td>\n",
       "      <td>(859 Reviews)</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Applied Data Scientist / ML Senior Engineer (P...</td>\n",
       "      <td>SAP India Pvt.Ltd</td>\n",
       "      <td>4.4</td>\n",
       "      <td>(1099 Reviews)</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>UPL</td>\n",
       "      <td>4.3</td>\n",
       "      <td>(1198 Reviews)</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Expert Data Scientist</td>\n",
       "      <td>UPL</td>\n",
       "      <td>4.3</td>\n",
       "      <td>(1198 Reviews)</td>\n",
       "      <td>6-9 Yrs</td>\n",
       "      <td>20,00,000 - 30,00,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>UPL</td>\n",
       "      <td>4.3</td>\n",
       "      <td>(1198 Reviews)</td>\n",
       "      <td>12-16 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Analyst - Applied Data Scientist</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>4.1</td>\n",
       "      <td>(859 Reviews)</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Research Data Scientist</td>\n",
       "      <td>Mavenir</td>\n",
       "      <td>3.9</td>\n",
       "      <td>(150 Reviews)</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist Grade12</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(9160 Reviews)</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Senior Data Scientist Grade12</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(9160 Reviews)</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job_Role            Company  \\\n",
       "1                               Senior Data Scientist                UPL   \n",
       "2                Sr. Analyst - Applied Data Scientist              Tesco   \n",
       "3   Applied Data Scientist / ML Senior Engineer (P...  SAP India Pvt.Ltd   \n",
       "4                                      Data Scientist                UPL   \n",
       "5                               Expert Data Scientist                UPL   \n",
       "6                            Principal Data Scientist                UPL   \n",
       "7                    Analyst - Applied Data Scientist              Tesco   \n",
       "8                             Research Data Scientist            Mavenir   \n",
       "9                       Senior Data Scientist Grade12           Flipkart   \n",
       "10                      Senior Data Scientist Grade12           Flipkart   \n",
       "\n",
       "   Ratings   No_of_Reviews Experience                     Salary  \\\n",
       "1      4.3  (1198 Reviews)    3-6 Yrs              Not disclosed   \n",
       "2      4.1   (859 Reviews)    3-5 Yrs              Not disclosed   \n",
       "3      4.4  (1099 Reviews)   5-10 Yrs              Not disclosed   \n",
       "4      4.3  (1198 Reviews)    1-4 Yrs              Not disclosed   \n",
       "5      4.3  (1198 Reviews)    6-9 Yrs  20,00,000 - 30,00,000 PA.   \n",
       "6      4.3  (1198 Reviews)  12-16 Yrs              Not disclosed   \n",
       "7      4.1   (859 Reviews)    1-2 Yrs              Not disclosed   \n",
       "8      3.9   (150 Reviews)    4-9 Yrs              Not disclosed   \n",
       "9      4.2  (9160 Reviews)   5-10 Yrs              Not disclosed   \n",
       "10     4.2  (9160 Reviews)    5-7 Yrs              Not disclosed   \n",
       "\n",
       "                                   Location  \n",
       "1   Bangalore/Bengaluru, Mumbai (All Areas)  \n",
       "2                       Bangalore/Bengaluru  \n",
       "3                       Bangalore/Bengaluru  \n",
       "4   Bangalore/Bengaluru, Mumbai (All Areas)  \n",
       "5   Bangalore/Bengaluru, Mumbai (All Areas)  \n",
       "6   Bangalore/Bengaluru, Mumbai (All Areas)  \n",
       "7                       Bangalore/Bengaluru  \n",
       "8                       Bangalore/Bengaluru  \n",
       "9                       Bangalore/Bengaluru  \n",
       "10                      Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(10)\n",
    "#Locate the search bar\n",
    "Job_name = driver.find_element_by_class_name('suggestor-input')\n",
    "\n",
    "#Enter the search text\n",
    "Job_name.send_keys('Data Scientist')\n",
    "time.sleep(10)\n",
    "#Enter the location bar\n",
    "Location = driver.find_element_by_class_name('locationSugg')\n",
    "\n",
    "#Enter the location text\n",
    "Location.find_element_by_class_name('suggestor-input ').send_keys('Bangalore')\n",
    "time.sleep(10)\n",
    "#Click on the Bangalore icon on the drop down\n",
    "driver.find_element_by_xpath('//div[@title=\"Bangalore/Bengaluru\"]').click()\n",
    "time.sleep(10)\n",
    "#click the crosswrapper\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "try:\n",
    "    driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
    "except NoSuchElementException as error:\n",
    "        pass\n",
    "time.sleep(10)    \n",
    "#click the search button\n",
    "driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
    "time.sleep(10)\n",
    "#extract the product card\n",
    "a = driver.find_elements_by_class_name('jobTupleHeader')\n",
    "\n",
    "#From the extracted data we find that totally there are 7 datas but for some cases there were no ratings & no of ratings\n",
    "# a template which was created for all cases,\n",
    "c,Job_Role=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "    else:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "\n",
    "\n",
    "c,Company=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "    else:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "\n",
    "#In this case,the order changes from here on, so change the else condition as not available for not matching the if ondition\n",
    "c,Ratings=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Ratings.append(a[i].text.split('\\n')[2])\n",
    "    else:\n",
    "        Ratings.append('Not available')\n",
    "        \n",
    "#In this case,the order changes, so change the else condition as not available for not matching the if ondition\n",
    "c,No_of_Reviews=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        No_of_Reviews.append(a[i].text.split('\\n')[3])\n",
    "    else:\n",
    "        No_of_Reviews.append('Not available')\n",
    "        \n",
    "#Change the order based on the previous cases\n",
    "c,Experience=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Experience.append(a[i].text.split('\\n')[4])\n",
    "    else:\n",
    "        Experience.append(a[i].text.split('\\n')[2])\n",
    "        \n",
    "c,Salary=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Salary.append(a[i].text.split('\\n')[5])\n",
    "    else:\n",
    "        Salary.append(a[i].text.split('\\n')[3])\n",
    "        \n",
    "c,Location=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Location.append(a[i].text.split('\\n')[6])\n",
    "    else:\n",
    "        Location.append(a[i].text.split('\\n')[4])\n",
    "        \n",
    "#import pandas for preparing dataframes\n",
    "import pandas as pd\n",
    "\n",
    "#Arrange the rows based on the category and zip it for the dataframe\n",
    "df_2 = pd.DataFrame(zip(Job_Role,Company,Ratings,No_of_Reviews,Experience,Salary,Location),columns=('Job_Role','Company','Ratings','No_of_Reviews','Experience','Salary','Location'))\n",
    "df_2.index = [x for x in range(1, len(df_2.values)+1)]\n",
    "#Since 10 results only required, we use iloc\n",
    "df_2.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb923a9",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below: You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs The task will be done as shown in the below steps:\n",
    "\n",
    "first get the webpage https://www.naukri.com/\n",
    "Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "Then click the search button.\n",
    "Then apply the location filter and salary filter by checking the respective boxes\n",
    "Then scrape the data for the first 10 jobs results you get.\n",
    "Finally create a dataframe of the scraped data. Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f23494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3234150472.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3234150472.py:11: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Job_name = driver.find_element_by_class_name('suggestor-input')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3234150472.py:19: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3234150472.py:24: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3234150472.py:28: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath(\".//*[contains(text(), 'Delhi / NCR')]\").click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3234150472.py:32: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath(\".//*[contains(text(), '3-6 Lakhs')]\").click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3234150472.py:35: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  a = driver.find_elements_by_class_name('jobTupleHeader')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Company</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>No_of_Reviews</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Internet Jobs - II</td>\n",
       "      <td>Jobs Territory</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>3.7</td>\n",
       "      <td>(22 Reviews)</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>EASY DATA ANALYTICS TECHNOLOGY PRIVATE LIMITED</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "      <td>4,00,000 - 6,00,000 PA.</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Associate Scientist - Data Engineering</td>\n",
       "      <td>AXA Technology Services India Pvt. Ltd</td>\n",
       "      <td>4.9</td>\n",
       "      <td>(3 Reviews)</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist || Software Company || Immediat...</td>\n",
       "      <td>Skyleaf Consultants</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist (freelance)</td>\n",
       "      <td>2Coms</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>4,75,000 - 9,75,000 PA.</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist/ Machine Learning, 2022 Passout...</td>\n",
       "      <td>Creative Hands HR Consultancy</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "      <td>5,00,000 - 7,00,000 PA.</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Delhi /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>BlackBuck</td>\n",
       "      <td>4.0</td>\n",
       "      <td>(935 Reviews)</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon, Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Associate Data Scientist</td>\n",
       "      <td>Optum</td>\n",
       "      <td>4.1</td>\n",
       "      <td>(1554 Reviews)</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hiring For Data Analyst and Data Scientist For...</td>\n",
       "      <td>Shadow Placements</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>5,00,000 - 13,00,000 PA.</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job_Role  \\\n",
       "1                 Data Scientist - Internet Jobs - II   \n",
       "2                                      Data Scientist   \n",
       "3                               Junior Data Scientist   \n",
       "4              Associate Scientist - Data Engineering   \n",
       "5   Data Scientist || Software Company || Immediat...   \n",
       "6                          Data Scientist (freelance)   \n",
       "7   Data Scientist/ Machine Learning, 2022 Passout...   \n",
       "8                                      Data Scientist   \n",
       "9                            Associate Data Scientist   \n",
       "10  Hiring For Data Analyst and Data Scientist For...   \n",
       "\n",
       "                                           Company        Ratings  \\\n",
       "1                                   Jobs Territory  Not available   \n",
       "2               Ashkom Media India Private Limited            3.7   \n",
       "3   EASY DATA ANALYTICS TECHNOLOGY PRIVATE LIMITED  Not available   \n",
       "4           AXA Technology Services India Pvt. Ltd            4.9   \n",
       "5                              Skyleaf Consultants  Not available   \n",
       "6                                            2Coms  Not available   \n",
       "7                    Creative Hands HR Consultancy  Not available   \n",
       "8                                        BlackBuck            4.0   \n",
       "9                                            Optum            4.1   \n",
       "10                               Shadow Placements  Not available   \n",
       "\n",
       "     No_of_Reviews Experience                    Salary  \\\n",
       "1    Not available    3-6 Yrs             Not disclosed   \n",
       "2     (22 Reviews)    3-6 Yrs             Not disclosed   \n",
       "3    Not available    1-2 Yrs   4,00,000 - 6,00,000 PA.   \n",
       "4      (3 Reviews)    2-5 Yrs             Not disclosed   \n",
       "5    Not available    3-8 Yrs             Not disclosed   \n",
       "6    Not available    2-7 Yrs   4,75,000 - 9,75,000 PA.   \n",
       "7    Not available    0-4 Yrs   5,00,000 - 7,00,000 PA.   \n",
       "8    (935 Reviews)    3-7 Yrs             Not disclosed   \n",
       "9   (1554 Reviews)    1-5 Yrs             Not disclosed   \n",
       "10   Not available    3-7 Yrs  5,00,000 - 13,00,000 PA.   \n",
       "\n",
       "                                             Location  \n",
       "1   Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...  \n",
       "2                          Noida, Bangalore/Bengaluru  \n",
       "3                                               Noida  \n",
       "4                                    Gurgaon/Gurugram  \n",
       "5               Gurgaon/Gurugram, Bangalore/Bengaluru  \n",
       "6                                    New Delhi, Delhi  \n",
       "7   Hyderabad/Secunderabad, Pune, Chennai, Delhi /...  \n",
       "8                                  Gurgaon, Bengaluru  \n",
       "9                                    Gurgaon/Gurugram  \n",
       "10               Noida, Gurgaon/Gurugram, Delhi / NCR  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(10)\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(10)\n",
    "#Locate the search bar\n",
    "Job_name = driver.find_element_by_class_name('suggestor-input')\n",
    "\n",
    "#Enter the search text\n",
    "Job_name.send_keys('Data Scientist')\n",
    "time.sleep(10)\n",
    "#click the crosswrapper\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "try:\n",
    "    driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
    "except NoSuchElementException as error:\n",
    "        pass\n",
    "time.sleep(10)    \n",
    "#click the search button\n",
    "driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
    "time.sleep(10)\n",
    "\n",
    "#Click on the bangalore dropdown using xpath\n",
    "driver.find_element_by_xpath(\".//*[contains(text(), 'Delhi / NCR')]\").click()\n",
    "time.sleep(10)\n",
    "\n",
    "#Click on the bangalore dropdown using xpath\n",
    "driver.find_element_by_xpath(\".//*[contains(text(), '3-6 Lakhs')]\").click()\n",
    "time.sleep(10)\n",
    "#extract the product card\n",
    "a = driver.find_elements_by_class_name('jobTupleHeader')\n",
    "\n",
    "#From the extracted data we find that totally there are 7 datas but for some cases there were no ratings & no of ratings\n",
    "# a template which was created for all cases,\n",
    "c,Job_Role=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "    else:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "        \n",
    "c,Company=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "    else:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "\n",
    "#In this case,the order changes from here on, so change the else condition as not available for not matching the if ondition\n",
    "c,Ratings=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Ratings.append(a[i].text.split('\\n')[2])\n",
    "    else:\n",
    "        Ratings.append('Not available')\n",
    "        \n",
    "#In this case,the order changes, so change the else condition as not available for not matching the if ondition\n",
    "c,No_of_Reviews=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        No_of_Reviews.append(a[i].text.split('\\n')[3])\n",
    "    else:\n",
    "        No_of_Reviews.append('Not available')\n",
    "        \n",
    "#Change the order based on the previous cases\n",
    "c,Experience=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Experience.append(a[i].text.split('\\n')[4])\n",
    "    else:\n",
    "        Experience.append(a[i].text.split('\\n')[2])\n",
    "        \n",
    "c,Salary=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Salary.append(a[i].text.split('\\n')[5])\n",
    "    else:\n",
    "        Salary.append(a[i].text.split('\\n')[3])\n",
    "        \n",
    "c,Location=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Location.append(a[i].text.split('\\n')[6])\n",
    "    else:\n",
    "        Location.append(a[i].text.split('\\n')[4])\n",
    "        \n",
    "#import pandas for preparing dataframes\n",
    "import pandas as pd\n",
    "\n",
    "#Arrange the rows based on the category and zip it for the dataframe\n",
    "df_3 = pd.DataFrame(zip(Job_Role,Company,Ratings,No_of_Reviews,Experience,Salary,Location),columns=('Job_Role','Company','Ratings','No_of_Reviews','Experience','Salary','Location'))\n",
    "df_3.index = [x for x in range(1, len(df_3.values)+1)]\n",
    "#Since 10 results only required, we use iloc\n",
    "df_3.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d6ed4",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "Brand\n",
    "Product Description\n",
    "Price The attributes which you have to scrape is ticked marked in the below image. To scrape the data you have to go through following steps:\n",
    "Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "Enter “sunglasses” in the search field where “search for products, brands andmore” is written and click the search icon\n",
    "After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the required data as usual. ASSIGNMENT 2\n",
    "After scraping data from the first page, go to the “Next” Button at the bottom ofthe page , then click on it.\n",
    "Now scrape data from this page as usual\n",
    "Repeat this until you get data for 100 sunglasses. Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f5b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/752271022.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/752271022.py:11: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  Login = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/752271022.py:14: DeprecationWarning: find_element_by_name is deprecated. Please use find_element(by=By.NAME, value=name) instead\n",
      "  driver.find_element_by_name('q').send_keys('sun glasses')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/752271022.py:17: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/752271022.py:20: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  page = driver.find_elements_by_class_name('ge-49M')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/752271022.py:37: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/752271022.py:39: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('_2B099V'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/752271022.py:61: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/752271022.py:61: DeprecationWarning: find_element_by_tag_name is deprecated. Please use find_element(by=By.TAG_NAME, value=name) instead\n",
      "  Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_Brand</th>\n",
       "      <th>Product_Price</th>\n",
       "      <th>Product_Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UV Protection Over-sized Sunglasses (64)</td>\n",
       "      <td>PIRASO</td>\n",
       "      <td>₹399</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient, Toughened Glass Lens, UV Protection ...</td>\n",
       "      <td>Singco India</td>\n",
       "      <td>₹630</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>₹647</td>\n",
       "      <td>28% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>₹639</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UV Protection, Polarized Wayfarer Sunglasses (...</td>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>₹283</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>UV Protection Over-sized Sunglasses (60)</td>\n",
       "      <td>PIRASO</td>\n",
       "      <td>₹399</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Gradient, Toughened Glass Lens, UV Protection ...</td>\n",
       "      <td>Singco India</td>\n",
       "      <td>₹630</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>UV Protection Aviator Sunglasses (56)</td>\n",
       "      <td>PIRASO</td>\n",
       "      <td>₹349</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>UV Protection, Mirrored Sports Sunglasses (62)</td>\n",
       "      <td>NuVew</td>\n",
       "      <td>₹345</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>PIRASO</td>\n",
       "      <td>₹224</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Product_Name Product_Brand  \\\n",
       "1             UV Protection Over-sized Sunglasses (64)        PIRASO   \n",
       "2    Gradient, Toughened Glass Lens, UV Protection ...  Singco India   \n",
       "3        UV Protection Wayfarer Sunglasses (Free Size)      Fastrack   \n",
       "4     UV Protection Rectangular Sunglasses (Free Size)      Fastrack   \n",
       "5    UV Protection, Polarized Wayfarer Sunglasses (...        SUNBEE   \n",
       "..                                                 ...           ...   \n",
       "96            UV Protection Over-sized Sunglasses (60)        PIRASO   \n",
       "97   Gradient, Toughened Glass Lens, UV Protection ...  Singco India   \n",
       "98               UV Protection Aviator Sunglasses (56)        PIRASO   \n",
       "99      UV Protection, Mirrored Sports Sunglasses (62)         NuVew   \n",
       "100              UV Protection Aviator Sunglasses (54)        PIRASO   \n",
       "\n",
       "    Product_Price Product_Discount  \n",
       "1            ₹399          84% off  \n",
       "2            ₹630          78% off  \n",
       "3            ₹647          28% off  \n",
       "4            ₹639          20% off  \n",
       "5            ₹283          78% off  \n",
       "..            ...              ...  \n",
       "96           ₹399          84% off  \n",
       "97           ₹630          78% off  \n",
       "98           ₹349          86% off  \n",
       "99           ₹345          72% off  \n",
       "100          ₹224          85% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(10)\n",
    "#click on the login close page\n",
    "Login = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "time.sleep(10)\n",
    "#locate the search bar give input\n",
    "driver.find_element_by_name('q').send_keys('sun glasses')\n",
    "\n",
    "#click on the search button\n",
    "driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
    "time.sleep(10)\n",
    "#extract  the page\n",
    "page = driver.find_elements_by_class_name('ge-49M')\n",
    "\n",
    "#extract the first ten pages\n",
    "link = []\n",
    "for i in page:\n",
    "    link.append(i.get_attribute('href'))\n",
    "time.sleep(10)    \n",
    "#since we need 100 results we need only first 3 pages\n",
    "a = link[0:3]\n",
    "time.sleep(10)\n",
    "#Extract the first three pages & its product data\n",
    "product = []\n",
    "for el in a:\n",
    "    #import selenium,webdriver,upload webdriver.exe\n",
    "    from selenium import webdriver as wb\n",
    "    import time\n",
    "    from selenium.webdriver.support.select import Select\n",
    "    driver = wb.Chrome('chromedriver.exe')\n",
    "    driver.get(el)\n",
    "    for i in driver.find_elements_by_class_name('_2B099V'):\n",
    "        product.append(i)\n",
    "        \n",
    "#Extract Product_brand by its class_name\n",
    "Product_Brand = []\n",
    "for i in product:\n",
    "    Product_Brand.append(i.find_elements_by_class_name('_2WkVRV')[0].text)\n",
    "\n",
    "#Extract Product_Name by its class_name\n",
    "Product_Name = []\n",
    "for i in product:\n",
    "    Product_Name.append(i.find_elements_by_class_name('IRpwTa')[0].get_attribute('title'))\n",
    "    \n",
    "#Extract Product_Name by its class_name\n",
    "Product_Price = []\n",
    "for i in product:\n",
    "    Product_Price.append(i.find_elements_by_class_name('_30jeq3')[0].text)\n",
    "\n",
    "\n",
    "#Extract Product_Name by its class_name\n",
    "Product_Discount = []\n",
    "for i in product:\n",
    "    Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n",
    "    \n",
    "# Import pandas to create pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Extract dataFrame\n",
    "df_4 = pd.DataFrame(zip(Product_Name,Product_Brand,Product_Price,Product_Discount),columns=(('Product_Name','Product_Brand','Product_Price','Product_Discount')))\n",
    "df_4.index = [x for x in range(1, len(df_4.values)+1)]\n",
    "#Extracted data for first 100 sunglasess\n",
    "df_4.iloc[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec4f6e",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace. When you will open the above link you will reach to the below shown webpage . As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "\n",
    "Rating\n",
    "Review summary\n",
    "Full review\n",
    "You have to scrape this data for first 100 reviews. Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5623db94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3818231203.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3818231203.py:11: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('_3TOw5k')[0].find_elements_by_tag_name('a')[14].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3818231203.py:11: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('_3TOw5k')[0].find_elements_by_tag_name('a')[14].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3818231203.py:15: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('ge-49M'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3818231203.py:24: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3818231203.py:26: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('_27M-vq'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3818231203.py:32: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  rating.append(i.find_element_by_class_name('row').text.split('\\n'))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3818231203.py:40: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Review_description.append(i.find_element_by_class_name('t-ZTKy').text.replace('\\n',''))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_star</th>\n",
       "      <th>Review_heading</th>\n",
       "      <th>Review_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️Its awesome mobile phone in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Smooth like butter, camera like fantabulous, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>It's my first time to use iOS phone and I am l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>A perfect phone and a good battery super camer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>This is my first ever iPhone.And I truly don’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Undoubtedly Iphone 11 is the most successful m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>I purchased the iPhone 11 a month back. I must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Don’t expect much from front camera… especiall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Review_star       Review_heading  \\\n",
       "1            5   Highly recommended   \n",
       "2            5     Perfect product!   \n",
       "3            5     Perfect product!   \n",
       "4            5    Worth every penny   \n",
       "5            5   Highly recommended   \n",
       "..         ...                  ...   \n",
       "86           5            Excellent   \n",
       "87           5               Super!   \n",
       "88           5    Worth every penny   \n",
       "89           5            Fabulous!   \n",
       "90           5  Best in the market!   \n",
       "\n",
       "                                   Review_description  \n",
       "1   iphone 11 is a very good phone to buy only if ...  \n",
       "2   It’s a must buy who is looking for an upgrade ...  \n",
       "3   Value for money❤️❤️Its awesome mobile phone in...  \n",
       "4   Smooth like butter, camera like fantabulous, s...  \n",
       "5   It's my first time to use iOS phone and I am l...  \n",
       "..                                                ...  \n",
       "86  A perfect phone and a good battery super camer...  \n",
       "87  This is my first ever iPhone.And I truly don’t...  \n",
       "88  Undoubtedly Iphone 11 is the most successful m...  \n",
       "89  I purchased the iPhone 11 a month back. I must...  \n",
       "90  Don’t expect much from front camera… especiall...  \n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(10)\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC')\n",
    "time.sleep(10)\n",
    "#find the next page url and click\n",
    "driver.find_elements_by_class_name('_3TOw5k')[0].find_elements_by_tag_name('a')[14].click()\n",
    "time.sleep(10)\n",
    "#Extract the first 10 pages\n",
    "nex = []\n",
    "for i in driver.find_elements_by_class_name('ge-49M'):\n",
    "    nex.append(i.get_attribute('href'))\n",
    "time.sleep(10)   \n",
    "#extract data from 10 pages and store it in review\n",
    "review = []\n",
    "for el in nex:\n",
    "    from selenium import webdriver as wb\n",
    "    import time\n",
    "    from selenium.webdriver.support.select import Select\n",
    "    driver = wb.Chrome('chromedriver.exe')\n",
    "    driver.get(el)\n",
    "    for i in driver.find_elements_by_class_name('_27M-vq'):\n",
    "        review.append(i)\n",
    "time.sleep(10)       \n",
    "# extract rating(star & header) \n",
    "rating = []\n",
    "for i in review:\n",
    "    rating.append(i.find_element_by_class_name('row').text.split('\\n'))\n",
    "    \n",
    "#seperate the star & heading\n",
    "review_star,review_heading = (list(zip(*rating)))\n",
    "\n",
    "# extract description using the class name\n",
    "Review_description = []\n",
    "for i in review:\n",
    "    Review_description.append(i.find_element_by_class_name('t-ZTKy').text.replace('\\n',''))\n",
    "    \n",
    "#import pandas for preapring dataframe\n",
    "import pandas as pd\n",
    "\n",
    "#inally zip with proper titles\n",
    "\n",
    "df_5 = pd.DataFrame(zip(review_star,review_heading,Review_description),columns=(('Review_star','Review_heading','Review_description')))                  \n",
    "df_5.index = [x for x in range(1, len(df_5.values)+1)]\n",
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94dc0394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_star</th>\n",
       "      <th>Review_heading</th>\n",
       "      <th>Review_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the MoneyThe iPhone 11 offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>A perfect phone and a good battery super camer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>This is my first ever iPhone.And I truly don’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Undoubtedly Iphone 11 is the most successful m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>I purchased the iPhone 11 a month back. I must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>Nice value for money good and best price I pho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Review_star       Review_heading  \\\n",
       "0            5            Brilliant   \n",
       "1            5       Simply awesome   \n",
       "2            5  Best in the market!   \n",
       "3            5     Perfect product!   \n",
       "4            5            Fabulous!   \n",
       "..         ...                  ...   \n",
       "95           5            Excellent   \n",
       "96           5               Super!   \n",
       "97           5    Worth every penny   \n",
       "98           5            Fabulous!   \n",
       "99           5            Wonderful   \n",
       "\n",
       "                                   Review_description  \n",
       "0   The Best Phone for the MoneyThe iPhone 11 offe...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  A perfect phone and a good battery super camer...  \n",
       "96  This is my first ever iPhone.And I truly don’t...  \n",
       "97  Undoubtedly Iphone 11 is the most successful m...  \n",
       "98  I purchased the iPhone 11 a month back. I must...  \n",
       "99  Nice value for money good and best price I pho...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558dfbf",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for “sneakers” in the search field. You have to scrape 4 attributes of each sneaker:\n",
    "\n",
    "Brand\n",
    "Product Description\n",
    "Price As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a98b3830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3319594789.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3319594789.py:11: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  Login = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3319594789.py:14: DeprecationWarning: find_element_by_name is deprecated. Please use find_element(by=By.NAME, value=name) instead\n",
      "  driver.find_element_by_name('q').send_keys('sneakers')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3319594789.py:17: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3319594789.py:20: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  page = driver.find_elements_by_class_name('ge-49M')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3319594789.py:37: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3319594789.py:39: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('_2B099V'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3319594789.py:60: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3319594789.py:60: DeprecationWarning: find_element_by_tag_name is deprecated. Please use find_element(by=By.TAG_NAME, value=name) instead\n",
      "  Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_Brand</th>\n",
       "      <th>Product_Price</th>\n",
       "      <th>Product_Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sneakers Sneakers For Men</td>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>₹158</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹245</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>HRX by Hrithik Roshan</td>\n",
       "      <td>₹692</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stylish Comfortable Lightweight, Breathable Wa...</td>\n",
       "      <td>BIRDE</td>\n",
       "      <td>₹269</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>Magnolia</td>\n",
       "      <td>₹351</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>ROCKFIELD</td>\n",
       "      <td>₹549</td>\n",
       "      <td>45% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>WATERPROOF-05cFULLWHITE Sneakers For Men</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>₹549</td>\n",
       "      <td>45% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Classy Sneakers For Men</td>\n",
       "      <td>Stefano Rads</td>\n",
       "      <td>₹201</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>WATERPROOF-05cFULLBLACK Sneakers For Men</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>₹549</td>\n",
       "      <td>45% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>₹424</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Product_Name          Product_Brand  \\\n",
       "1                            Sneakers Sneakers For Men               URBANBOX   \n",
       "2        Modern Trendy Sneakers Shoes Sneakers For Men                 BRUTON   \n",
       "3                                     Sneakers For Men  HRX by Hrithik Roshan   \n",
       "4    Stylish Comfortable Lightweight, Breathable Wa...                  BIRDE   \n",
       "5                                     Sneakers For Men               Magnolia   \n",
       "..                                                 ...                    ...   \n",
       "96                                    Sneakers For Men              ROCKFIELD   \n",
       "97            WATERPROOF-05cFULLWHITE Sneakers For Men                  ASIAN   \n",
       "98                             Classy Sneakers For Men           Stefano Rads   \n",
       "99            WATERPROOF-05cFULLBLACK Sneakers For Men                  ASIAN   \n",
       "100                                   Sneakers For Men           Robbie jones   \n",
       "\n",
       "    Product_Price Product_Discount  \n",
       "1            ₹158          84% off  \n",
       "2            ₹245          81% off  \n",
       "3            ₹692          80% off  \n",
       "4            ₹269          73% off  \n",
       "5            ₹351          64% off  \n",
       "..            ...              ...  \n",
       "96           ₹549          45% off  \n",
       "97           ₹549          45% off  \n",
       "98           ₹201          71% off  \n",
       "99           ₹549          45% off  \n",
       "100          ₹424          57% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(10)\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(10)\n",
    "#click on the login close page\n",
    "Login = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "time.sleep(10)\n",
    "#locate the search bar give input\n",
    "driver.find_element_by_name('q').send_keys('sneakers')\n",
    "time.sleep(10)\n",
    "#click on the search button\n",
    "driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
    "time.sleep(10)\n",
    "#extract  the page\n",
    "page = driver.find_elements_by_class_name('ge-49M')\n",
    "time.sleep(10)\n",
    "#extract the first ten pages\n",
    "link = []\n",
    "for i in page:\n",
    "    link.append(i.get_attribute('href'))\n",
    "time.sleep(10)    \n",
    "#since we need 100 results we need only first 3 pages\n",
    "a = link[0:3]\n",
    "time.sleep(10)\n",
    "#Extract the first three pages & its product data\n",
    "product = []\n",
    "for el in a:\n",
    "    #import selenium,webdriver,upload webdriver.exe\n",
    "    from selenium import webdriver as wb\n",
    "    import time\n",
    "    from selenium.webdriver.support.select import Select\n",
    "    driver = wb.Chrome('chromedriver.exe')\n",
    "    driver.get(el)\n",
    "    for i in driver.find_elements_by_class_name('_2B099V'):\n",
    "        product.append(i)\n",
    "time.sleep(10)        \n",
    "#Extract Product_brand by its class_name\n",
    "Product_Brand = []\n",
    "for i in product:\n",
    "    Product_Brand.append(i.find_elements_by_class_name('_2WkVRV')[0].text)\n",
    "    \n",
    "#Extract Product_Name by its class_name\n",
    "Product_Name = []\n",
    "for i in product:\n",
    "    Product_Name.append(i.find_elements_by_class_name('IRpwTa')[0].get_attribute('title'))\n",
    "    \n",
    "#Extract Product_Name by its class_name\n",
    "Product_Price = []\n",
    "for i in product:\n",
    "    Product_Price.append(i.find_elements_by_class_name('_30jeq3')[0].text)\n",
    "    \n",
    "#Extract Product_Name by its class_name\n",
    "Product_Discount = []\n",
    "for i in product:\n",
    "    Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n",
    "    \n",
    "# Import pandas to create pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Extract dataFrame\n",
    "df_6 = pd.DataFrame(zip(Product_Name,Product_Brand,Product_Price,Product_Discount),columns=(('Product_Name','Product_Brand','Product_Price','Product_Discount')))\n",
    "df_6.index = [x for x in range(1, len(df_6.values)+1)]\n",
    "#Extracted data for first 100 sunglasess\n",
    "df_6.iloc[0:100:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c8991",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 7149 to Rs. 14099 ” , Color filter to “Black”, as shown inthe below image. And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18159a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:11: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//li[@class=\"colour-listItem\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:14: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_element_by_class_name('price-list').find_elements_by_tag_name('li')[1].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:14: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  driver.find_element_by_class_name('price-list').find_elements_by_tag_name('li')[1].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:17: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('product-productMetaInfo')[0].text\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:20: DeprecationWarning: find_element_by_link_text is deprecated. Please use find_element(by=By.LINK_TEXT, value=link_text) instead\n",
      "  page = driver.find_element_by_link_text('2').get_attribute('href')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:36: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:40: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('product-base'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:46: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Product_Name.append(a[i].find_element_by_class_name('product-product').text.split('\\n')[0])\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2943841544.py:51: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Product_Brand.append(a[i].find_element_by_class_name('product-brand').text.split('\\n')[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_Brand</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Men Max Cushioning Running</td>\n",
       "      <td>Skechers</td>\n",
       "      <td>7199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Men Sneakers</td>\n",
       "      <td>ALDO</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Men React Miler2 Running Shoes</td>\n",
       "      <td>Nike</td>\n",
       "      <td>11495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Men Charged Rouge 3 Run Shoes</td>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Women React MR 3 Running Shoes</td>\n",
       "      <td>Nike</td>\n",
       "      <td>8920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Women Textured Mules</td>\n",
       "      <td>Bugatti</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Men Leather Formal Loafers</td>\n",
       "      <td>J.FONTINI</td>\n",
       "      <td>7490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Block Pumps with Bows</td>\n",
       "      <td>ALDO</td>\n",
       "      <td>8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Women Leather Loafers</td>\n",
       "      <td>Cole Haan</td>\n",
       "      <td>11899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Wedge Sandals</td>\n",
       "      <td>ALDO</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Product_Name Product_Brand    Rate\n",
       "1        Men Max Cushioning Running      Skechers    7199\n",
       "2                      Men Sneakers          ALDO    9999\n",
       "3    Men React Miler2 Running Shoes          Nike   11495\n",
       "4     Men Charged Rouge 3 Run Shoes  UNDER ARMOUR    7999\n",
       "5    Women React MR 3 Running Shoes          Nike    8920\n",
       "..                              ...           ...     ...\n",
       "96             Women Textured Mules       Bugatti    7999\n",
       "97       Men Leather Formal Loafers     J.FONTINI    7490\n",
       "98            Block Pumps with Bows          ALDO    8999\n",
       "99            Women Leather Loafers     Cole Haan   11899\n",
       "100                   Wedge Sandals          ALDO    7999\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(10)\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.myntra.com/shoes')\n",
    "time.sleep(10)\n",
    "#Select the colour checklist and click\n",
    "driver.find_element_by_xpath('//li[@class=\"colour-listItem\"]').click()\n",
    "time.sleep(10)\n",
    "#Select the price checklist and click\n",
    "driver.find_element_by_class_name('price-list').find_elements_by_tag_name('li')[1].click()\n",
    "time.sleep(10)\n",
    "#extract the product data\n",
    "driver.find_elements_by_class_name('product-productMetaInfo')[0].text\n",
    "time.sleep(10)\n",
    "#extract page url\n",
    "page = driver.find_element_by_link_text('2').get_attribute('href')\n",
    "\n",
    "time.sleep(10)#extract all pages\n",
    "li = []\n",
    "for i in range(1,19):\n",
    "    li.append(page[:-1]+str(i))\n",
    "time.sleep(10)    \n",
    "#As we need 100 products,we extract only 3 pages\n",
    "k =li[0:2]\n",
    "time.sleep(10)\n",
    "#extract data from 3 pages\n",
    "a = []\n",
    "for el in k:\n",
    "    from selenium import webdriver\n",
    "    import time\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(el)\n",
    "\n",
    "\n",
    "    for i in driver.find_elements_by_class_name('product-base'):\n",
    "        a.append(i)\n",
    "time.sleep(10)        \n",
    "#Extract product name info\n",
    "Product_Name=[]\n",
    "for i in range(100):\n",
    "    Product_Name.append(a[i].find_element_by_class_name('product-product').text.split('\\n')[0])\n",
    "    \n",
    "#Extract product brand info\n",
    "Product_Brand=[]\n",
    "for i in range(100):\n",
    "    Product_Brand.append(a[i].find_element_by_class_name('product-brand').text.split('\\n')[0])\n",
    "\n",
    "#Extract product price info\n",
    "Product_Price=[]\n",
    "for i in range(100):\n",
    "    Product_Price.append(a[i].find_elements_by_class_name('product-price')[0].text)\n",
    "    #dataframe for 100 products\n",
    "    \n",
    "    \n",
    "#Extract product rate info\n",
    "Rate = []\n",
    "for i in range(100):\n",
    "    Rate.append(Product_Price[i].split('Rs.')[1])\n",
    "    \n",
    "#import pandas for preparing dataframe\n",
    "import pandas as pd\n",
    "df_7 = pd.DataFrame(zip(Product_Name,Product_Brand,Rate),columns=('Product_Name','Product_Brand','Rate'))\n",
    "df_7.index = [x for x in range(1, len(df_7.values)+1)]\n",
    "#only for 100 products so we use iloc\n",
    "df_7.iloc[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab35081f",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fda81948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/213328120.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/213328120.py:11: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('twotabsearchtextbox').send_keys('Laptop')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/213328120.py:14: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('nav-search-submit-button').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/213328120.py:17: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('p_n_feature_thirteen_browse-bin/12598163031').find_element_by_tag_name('span').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/213328120.py:17: DeprecationWarning: find_element_by_tag_name is deprecated. Please use find_element(by=By.TAG_NAME, value=name) instead\n",
      "  driver.find_element_by_id('p_n_feature_thirteen_browse-bin/12598163031').find_element_by_tag_name('span').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/213328120.py:20: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  d = driver.find_elements_by_xpath('//div[@class=\"a-section a-spacing-small a-spacing-top-small\"]')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/213328120.py:39: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  name.append(d[i].find_element_by_xpath('*//span[@class=\"a-size-medium a-color-base a-text-normal\"]').text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/213328120.py:48: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  price.append(d[i].find_element_by_class_name('a-price').text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_Ratings</th>\n",
       "      <th>Product_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy Book2 Pro 360 Intel 12th Gen i7...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>₹1,29,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSI Katana GF66 Gaming, Intel i7-11800H, 15.6\"...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>₹91,499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mi Notebook Ultra 3.2K Resolution Display Inte...</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>₹77,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>₹57,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP Pavilion x360 11th Gen Intel Core i7 14 inc...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>₹85,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Samsung Galaxy Book2 Intel 12th Gen core i7 39...</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "      <td>₹79,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>₹89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP Pavilion 14, 11th Gen Intel Core i7-16GB RA...</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>₹86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LG Gram Intel Evo 11th Gen Core i7 17 inches U...</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>₹93,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fujitsu UH-X 11th Gen Intel Core i7 13.3” FHD ...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>₹84,590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Product_Name     Product_Ratings  \\\n",
       "1   Samsung Galaxy Book2 Pro 360 Intel 12th Gen i7...  5.0 out of 5 stars   \n",
       "2   MSI Katana GF66 Gaming, Intel i7-11800H, 15.6\"...  5.0 out of 5 stars   \n",
       "3   Mi Notebook Ultra 3.2K Resolution Display Inte...  4.3 out of 5 stars   \n",
       "4   ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...  4.4 out of 5 stars   \n",
       "5   HP Pavilion x360 11th Gen Intel Core i7 14 inc...  4.0 out of 5 stars   \n",
       "6   Samsung Galaxy Book2 Intel 12th Gen core i7 39...  3.0 out of 5 stars   \n",
       "7   ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...  4.6 out of 5 stars   \n",
       "8   HP Pavilion 14, 11th Gen Intel Core i7-16GB RA...  4.3 out of 5 stars   \n",
       "9   LG Gram Intel Evo 11th Gen Core i7 17 inches U...  4.5 out of 5 stars   \n",
       "10  Fujitsu UH-X 11th Gen Intel Core i7 13.3” FHD ...  4.4 out of 5 stars   \n",
       "\n",
       "   Product_Price  \n",
       "1      ₹1,29,990  \n",
       "2        ₹91,499  \n",
       "3        ₹77,999  \n",
       "4        ₹57,490  \n",
       "5        ₹85,890  \n",
       "6        ₹79,990  \n",
       "7        ₹89,990  \n",
       "8        ₹86,990  \n",
       "9        ₹93,999  \n",
       "10       ₹84,590  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium wedriver and time\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#use chromedriver\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.amazon.in/')\n",
    "time.sleep(10)\n",
    "#give input\n",
    "driver.find_element_by_id('twotabsearchtextbox').send_keys('Laptop')\n",
    "time.sleep(10)\n",
    "#click on search button\n",
    "driver.find_element_by_id('nav-search-submit-button').click()\n",
    "time.sleep(10)\n",
    "#click on i7 checkbox button\n",
    "driver.find_element_by_id('p_n_feature_thirteen_browse-bin/12598163031').find_element_by_tag_name('span').click()\n",
    "time.sleep(10)\n",
    "#extract product data\n",
    "d = driver.find_elements_by_xpath('//div[@class=\"a-section a-spacing-small a-spacing-top-small\"]')\n",
    "time.sleep(10)                                  \n",
    "\n",
    "#extract product ratings,use exception due to unvavailabilty of data\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "ratings = []\n",
    "for i in range(31):\n",
    "    try:\n",
    "        ratings.append(d[i].find_elements_by_class_name('a-icon-alt')[0].get_attribute('innerHTML'))\n",
    "    except (IndexError,NoSuchElementException) as error:\n",
    "        ratings.append('Not available')\n",
    "\n",
    "\n",
    "        \n",
    "#extract product name,use exception due to unvavailabilty of data\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "name = []\n",
    "for i in range(31):\n",
    "    try:\n",
    "        name.append(d[i].find_element_by_xpath('*//span[@class=\"a-size-medium a-color-base a-text-normal\"]').text)\n",
    "    except (IndexError,NoSuchElementException) as error:\n",
    "        name.append('Not available')\n",
    "        \n",
    "#extract product price,use exception due to unvavailabilty of data\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "price = []\n",
    "for i in range(31):\n",
    "    try:\n",
    "        price.append(d[i].find_element_by_class_name('a-price').text)\n",
    "    except (IndexError,NoSuchElementException) as error:\n",
    "        price.append('Not available')\n",
    "        \n",
    "#extract product ratings,use exception due to unvavailabilty of data\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "ratings = []\n",
    "for i in range(31):\n",
    "    try:\n",
    "        ratings.append(d[i].find_elements_by_class_name('a-icon-alt')[0].get_attribute('innerHTML'))\n",
    "    except (IndexError,NoSuchElementException) as error:\n",
    "        ratings.append('Not available')\n",
    "                                  \n",
    "#import pandas and extract data frame for 10 products\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(zip(name,ratings,price),columns = ('Product_Name','Product_Ratings','Product_Price'))\n",
    "df_8.index = [x for x in range(1, len(df_8.values)+1)]\n",
    "df_8.iloc[0:10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f361ac4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_Ratings</th>\n",
       "      <th>Product_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI Modern 14, Intel i7-1195G7, 14\" FHD IPS-Le...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>₹76,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samsung Galaxy Book2 Pro 360 Intel 12th Gen i7...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>₹1,29,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mi Notebook Ultra 3.2K Resolution Display Inte...</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>₹77,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>₹86,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>₹57,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>₹89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HP Pavilion 15 12th Gen Intel Core i7 16GB SDR...</td>\n",
       "      <td>Not available</td>\n",
       "      <td>₹92,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Samsung Galaxy Book2 Intel 12th Gen core i7 39...</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "      <td>₹79,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HP Pavilion x360 11th Gen Intel Core i7 14 inc...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>₹85,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Acer Predator Helios 300 11th Gen Intel Core i...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>₹1,69,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Product_Name     Product_Ratings  \\\n",
       "1   MSI Modern 14, Intel i7-1195G7, 14\" FHD IPS-Le...  4.0 out of 5 stars   \n",
       "2   Samsung Galaxy Book2 Pro 360 Intel 12th Gen i7...  5.0 out of 5 stars   \n",
       "3   Mi Notebook Ultra 3.2K Resolution Display Inte...  4.3 out of 5 stars   \n",
       "4   Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....  4.4 out of 5 stars   \n",
       "5   ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...  4.4 out of 5 stars   \n",
       "6   ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...  4.6 out of 5 stars   \n",
       "7   HP Pavilion 15 12th Gen Intel Core i7 16GB SDR...       Not available   \n",
       "8   Samsung Galaxy Book2 Intel 12th Gen core i7 39...  3.0 out of 5 stars   \n",
       "9   HP Pavilion x360 11th Gen Intel Core i7 14 inc...  4.0 out of 5 stars   \n",
       "10  Acer Predator Helios 300 11th Gen Intel Core i...  5.0 out of 5 stars   \n",
       "\n",
       "   Product_Price  \n",
       "1        ₹76,490  \n",
       "2      ₹1,29,990  \n",
       "3        ₹77,999  \n",
       "4        ₹86,900  \n",
       "5        ₹57,490  \n",
       "6        ₹89,990  \n",
       "7        ₹92,400  \n",
       "8        ₹79,990  \n",
       "9        ₹85,890  \n",
       "10     ₹1,69,990  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20745712",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. This task will be done in following steps:\n",
    "\n",
    "First get the webpage https://www.ambitionbox.com/\n",
    "Click on the Job option as shown in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b01b00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:11: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_elements_by_xpath('//*[@id=\"headerWrapper\"]/nav/nav/a[6]')[0].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:14: DeprecationWarning: find_element_by_name is deprecated. Please use find_element(by=By.NAME, value=name) instead\n",
      "  driver.find_element_by_name('ab_jobsSearch').send_keys('Data Scientist')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:17: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_element_by_class_name('ctas-btn-medium').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:20: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('icon-chevron-right')[1].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:23: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('location_Noida').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:26: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('text-center')[0].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:29: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  other = driver.find_elements_by_class_name('other-info')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:38: DeprecationWarning: find_elements_by_css_selector is deprecated. Please use find_elements(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  for i in driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:39: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Job_Role.append(i.find_elements_by_tag_name('h2')[0].text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:43: DeprecationWarning: find_elements_by_css_selector is deprecated. Please use find_elements(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  for i in driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:44: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Company.append(i.find_elements_by_tag_name('p')[0].text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:48: DeprecationWarning: find_elements_by_css_selector is deprecated. Please use find_elements(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  a = driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:51: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  h = a[2].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:53: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  c = len(a[1].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p'))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:58: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  b.append(len(i.find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:61: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Salary.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[1].text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:68: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  b.append(len(i.find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:71: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Location.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[2].text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/2112546139.py:73: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Location.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[1].text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_role</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist- AI/ML- R&amp;D</td>\n",
       "      <td>Chennai, Pune, Delhi NCR +4 more</td>\n",
       "      <td>₹ 6.5-16.5 LPA</td>\n",
       "      <td>EXL Services.com ( I ) Pvt. Ltd.</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bengaluru/Bangalore, Hyderabad/Secunderabad, G...</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oracle HCM BI Technical Lead/Manager (People A...</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>TECHNIP GLOBAL BUSINESS SERVICES PRIVATE LIMITED</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring For Data Scientist + Python/R+ Predicti...</td>\n",
       "      <td>Gurgaon/Gurugram, Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>9-14 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Immediate Joiners</td>\n",
       "      <td>Pune, Mumbai, Bengaluru/Bangalore +1 more</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Bristlecone India Limited</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Machine Learning (5-14 yrs)</td>\n",
       "      <td>Bengaluru/Bangalore, Noida</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Zyoin</td>\n",
       "      <td>5-14 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist- Fresher Opening - Newgen Softw...</td>\n",
       "      <td>Noida</td>\n",
       "      <td>₹ 3-5 LPA</td>\n",
       "      <td>Newgen Software Technologies Ltd.</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bengaluru/Bangalore, Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Pitney Bowes India Pvt Ltd</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job_role  \\\n",
       "1                                      Data Scientist   \n",
       "2                          Data Scientist- AI/ML- R&D   \n",
       "3                                      Data Scientist   \n",
       "4   Oracle HCM BI Technical Lead/Manager (People A...   \n",
       "5   Hiring For Data Scientist + Python/R+ Predicti...   \n",
       "6                  Data Scientist - Immediate Joiners   \n",
       "7        Data Scientist - Machine Learning (5-14 yrs)   \n",
       "8   Data Scientist- Fresher Opening - Newgen Softw...   \n",
       "9                                      Data Scientist   \n",
       "10                              Senior Data Scientist   \n",
       "\n",
       "                                             Location          Salary  \\\n",
       "1                                               Noida   Not Disclosed   \n",
       "2                    Chennai, Pune, Delhi NCR +4 more  ₹ 6.5-16.5 LPA   \n",
       "3   Bengaluru/Bangalore, Hyderabad/Secunderabad, G...   Not Disclosed   \n",
       "4                                               Noida   Not Disclosed   \n",
       "5                             Gurgaon/Gurugram, Noida   Not Disclosed   \n",
       "6           Pune, Mumbai, Bengaluru/Bangalore +1 more   Not Disclosed   \n",
       "7                          Bengaluru/Bangalore, Noida   Not Available   \n",
       "8                                               Noida       ₹ 3-5 LPA   \n",
       "9                          Bengaluru/Bangalore, Noida   Not Disclosed   \n",
       "10                                              Noida   Not Disclosed   \n",
       "\n",
       "                                             Company Experience  \n",
       "1           Ericsson India Global Services Pvt. Ltd.    5-8 Yrs  \n",
       "2                   EXL Services.com ( I ) Pvt. Ltd.    2-6 Yrs  \n",
       "3                      GENPACT India Private Limited   7-12 Yrs  \n",
       "4   TECHNIP GLOBAL BUSINESS SERVICES PRIVATE LIMITED   7-12 Yrs  \n",
       "5                      GENPACT India Private Limited   9-14 Yrs  \n",
       "6                          Bristlecone India Limited    5-8 Yrs  \n",
       "7                                              Zyoin   5-14 Yrs  \n",
       "8                  Newgen Software Technologies Ltd.    0-2 Yrs  \n",
       "9                 Ashkom Media India Private Limited    3-6 Yrs  \n",
       "10                        Pitney Bowes India Pvt Ltd   6-10 Yrs  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium webdriver & time\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#extract data from url\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "time.sleep(10)\n",
    "#click on jobs\n",
    "driver.find_elements_by_xpath('//*[@id=\"headerWrapper\"]/nav/nav/a[6]')[0].click()\n",
    "time.sleep(5)\n",
    "#Enter Data Scientist on the seatch bar\n",
    "driver.find_element_by_name('ab_jobsSearch').send_keys('Data Scientist')\n",
    "time.sleep(5)\n",
    "#click on the seatch button\n",
    "driver.find_element_by_class_name('ctas-btn-medium').click()\n",
    "time.sleep(5)\n",
    "#click search button on the location\n",
    "driver.find_elements_by_class_name('icon-chevron-right')[1].click()\n",
    "time.sleep(5)\n",
    "#Select noida location\n",
    "driver.find_element_by_id('location_Noida').click()\n",
    "time.sleep(5)\n",
    "#click on load more jobs\n",
    "driver.find_elements_by_class_name('text-center')[0].click()\n",
    "time.sleep(5)\n",
    "#find the job text\n",
    "other = driver.find_elements_by_class_name('other-info')\n",
    "\n",
    "#Extract job Experience\n",
    "Experience=[]\n",
    "for i in range(len(other)):\n",
    "    Experience.append(other[i].text.split('\\n')[0])\n",
    "\n",
    "#Extract job role\n",
    "Job_Role=[]\n",
    "for i in driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]'):\n",
    "    Job_Role.append(i.find_elements_by_tag_name('h2')[0].text)\n",
    "    \n",
    "#Extract job Company\n",
    "Company=[]\n",
    "for i in driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]'):\n",
    "    Company.append(i.find_elements_by_tag_name('p')[0].text)\n",
    "    \n",
    "#job ratings varies for each jpb post,counting the no of attributes in each post\n",
    "\n",
    "a = driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]')\n",
    "\n",
    "#We find each post has around 4 attributes with ratings\n",
    "h = a[2].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')\n",
    "\n",
    "c = len(a[1].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p'))\n",
    "\n",
    "#Using exception extract salary info\n",
    "b,Salary=[],[]\n",
    "for i in a:\n",
    "    b.append(len(i.find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')))\n",
    "for i in range(20):\n",
    "    if b[i]==c:\n",
    "        Salary.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[1].text)\n",
    "    else:\n",
    "        Salary.append('Not Available')\n",
    "        \n",
    "#Using exception extract location info\n",
    "b,Location=[],[]\n",
    "for i in a:\n",
    "    b.append(len(i.find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')))\n",
    "for i in range(20):\n",
    "    if b[i]==4:\n",
    "        Location.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[2].text)\n",
    "    else:\n",
    "        Location.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[1].text)\n",
    "        \n",
    "#Extract dataframe by importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "df_9 = pd.DataFrame(zip(Job_Role,Location,Salary,Company,Experience),columns=('Job_role','Location','Salary','Company','Experience'))\n",
    "df_9.index = [x for x in range(1, len(df_9.values)+1)]\n",
    "#extracting data for 10 posts\n",
    "df_9.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc09ce",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation. You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary. The above task will be, done as shown in the below steps:\n",
    "\n",
    "First get the webpage https://www.ambitionbox.com/\n",
    "Click on the salaries option as shown in the image.\n",
    "After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”. You have to scrape the data ticked in the above image.\n",
    "Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "Store the data in a dataframe. Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "071ea171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3789109197.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3789109197.py:11: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  driver.find_elements_by_tag_name('a')[4].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3789109197.py:14: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('jobProfileSearchbox').send_keys('Data Scientist')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3789109197.py:17: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('icon-search')[0].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3789109197.py:20: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  a = driver.find_elements_by_class_name('company-info-wrapper')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3789109197.py:23: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  r = len(driver.find_elements_by_class_name('company-info-wrapper'))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_15460/3789109197.py:34: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  s = driver.find_elements_by_class_name('salary-range-wrapper')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>No_of_Salaries</th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Lowest_Salary</th>\n",
       "      <th>Average_Salary</th>\n",
       "      <th>Highest_Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google</td>\n",
       "      <td>based on 21 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 10.4L</td>\n",
       "      <td>₹ 33.1L</td>\n",
       "      <td>₹ 97.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arista Networks</td>\n",
       "      <td>based on 13 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 10.7L</td>\n",
       "      <td>₹ 24.7L</td>\n",
       "      <td>₹ 38.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft Corporation</td>\n",
       "      <td>based on 255 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 13.0L</td>\n",
       "      <td>₹ 23.7L</td>\n",
       "      <td>₹ 45.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>based on 17 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 12.0L</td>\n",
       "      <td>₹ 22.7L</td>\n",
       "      <td>₹ 34.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tekion</td>\n",
       "      <td>based on 33 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 12.0L</td>\n",
       "      <td>₹ 21.0L</td>\n",
       "      <td>₹ 33.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>based on 108 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 8.0L</td>\n",
       "      <td>₹ 20.6L</td>\n",
       "      <td>₹ 45.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Flipkart</td>\n",
       "      <td>based on 62 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 7.5L</td>\n",
       "      <td>₹ 20.4L</td>\n",
       "      <td>₹ 31.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PayPal</td>\n",
       "      <td>based on 27 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 12.0L</td>\n",
       "      <td>₹ 20.2L</td>\n",
       "      <td>₹ 31.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Servicenow Software Development India</td>\n",
       "      <td>based on 54 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 13.0L</td>\n",
       "      <td>₹ 20.2L</td>\n",
       "      <td>₹ 26.3L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>based on 82 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 19.8L</td>\n",
       "      <td>₹ 32.5L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Company         No_of_Salaries  \\\n",
       "1                                  Google   based on 21 salaries   \n",
       "2                         Arista Networks   based on 13 salaries   \n",
       "3                   Microsoft Corporation  based on 255 salaries   \n",
       "4                           Goldman Sachs   based on 17 salaries   \n",
       "5                                  Tekion   based on 33 salaries   \n",
       "6                                  Amazon  based on 108 salaries   \n",
       "7                                Flipkart   based on 62 salaries   \n",
       "8                                  PayPal   based on 27 salaries   \n",
       "9   Servicenow Software Development India   based on 54 salaries   \n",
       "10                                Walmart   based on 82 salaries   \n",
       "\n",
       "             Job_Role Lowest_Salary Average_Salary Highest_Salary  \n",
       "1   Software Engineer       ₹ 10.4L        ₹ 33.1L        ₹ 97.0L  \n",
       "2   Software Engineer       ₹ 10.7L        ₹ 24.7L        ₹ 38.0L  \n",
       "3   Software Engineer       ₹ 13.0L        ₹ 23.7L        ₹ 45.0L  \n",
       "4   Software Engineer       ₹ 12.0L        ₹ 22.7L        ₹ 34.0L  \n",
       "5   Software Engineer       ₹ 12.0L        ₹ 21.0L        ₹ 33.0L  \n",
       "6   Software Engineer        ₹ 8.0L        ₹ 20.6L        ₹ 45.0L  \n",
       "7   Software Engineer        ₹ 7.5L        ₹ 20.4L        ₹ 31.0L  \n",
       "8   Software Engineer       ₹ 12.0L        ₹ 20.2L        ₹ 31.0L  \n",
       "9   Software Engineer       ₹ 13.0L        ₹ 20.2L        ₹ 26.3L  \n",
       "10  Software Engineer       ₹ 11.0L        ₹ 19.8L        ₹ 32.5L  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium, webdriver time\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#extract url\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "time.sleep(3)\n",
    "# Click on the salaries option\n",
    "driver.find_elements_by_tag_name('a')[4].click()\n",
    "time.sleep(3)\n",
    "#After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist”  \n",
    "driver.find_element_by_id('jobProfileSearchbox').send_keys('Data Scientist')\n",
    "time.sleep(3)\n",
    "#then click on “Data Scientist\n",
    "driver.find_elements_by_class_name('icon-search')[0].click()\n",
    "time.sleep(3)\n",
    "#Extract job profile data\n",
    "a = driver.find_elements_by_class_name('company-info-wrapper')\n",
    "time.sleep(3)\n",
    "#Calculate the length of product data,some dont have review rating\n",
    "r = len(driver.find_elements_by_class_name('company-info-wrapper'))\n",
    "\n",
    "#now split for all posts\n",
    "b = []\n",
    "for i in range(r):\n",
    "    b.append(a[i].text.split('\\n'))\n",
    "    \n",
    "#map using zip option\n",
    "Company,No_of_Salaries,Job_Role,Dot,Experience = list(zip(*b))\n",
    "\n",
    "#Extract salary info\n",
    "s = driver.find_elements_by_class_name('salary-range-wrapper')\n",
    "\n",
    "#Extract ranges of salary\n",
    "d = []\n",
    "for i in range(10):\n",
    "    d.append(s[i].text.split('\\n'))\n",
    "    \n",
    "#map using zip option\n",
    "Average_Salary,Lowest_Salary,Highest_Salary = list(zip(*d))\n",
    "\n",
    "#Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Extract datafame\n",
    "df_10 = pd.DataFrame(zip(Company,No_of_Salaries,Job_Role,Lowest_Salary,Average_Salary,Highest_Salary),columns=('Company','No_of_Salaries','Job_Role','Lowest_Salary','Average_Salary','Highest_Salary'))\n",
    "df_10.index = [x for x in range(1, len(df_10.values)+1)]\n",
    "#data for the first 10 posts\n",
    "df_10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c030512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
