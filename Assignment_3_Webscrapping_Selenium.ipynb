{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f6784c",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79cbf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2071231604.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2071231604.py:13: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Job_name = driver.find_element_by_class_name('suggestor-input')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2071231604.py:19: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Location = driver.find_element_by_class_name('locationSugg')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2071231604.py:22: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Location.find_element_by_class_name('suggestor-input ').send_keys('Bangalore/Bengaluru')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2071231604.py:29: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2071231604.py:35: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2071231604.py:39: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  a = driver.find_elements_by_class_name('jobTupleHeader')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Company</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>No_of_Reviews</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>METRO Cash &amp; Carry</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(957 Reviews)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>15,00,000 - 27,50,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>METRO Cash &amp; Carry</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(957 Reviews)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>15,00,000 - 27,50,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data analyst/ data analytics / Business analys...</td>\n",
       "      <td>Leading US MNC into Analytics</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>10,00,000 - 20,00,000 PA.</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Delhi /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business analyst + data Analysis</td>\n",
       "      <td>Anlage Infotech (I) Pvt. Ltd.</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>upGrad</td>\n",
       "      <td>3.1</td>\n",
       "      <td>(337 Reviews)</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Associate Professional Data Analyst</td>\n",
       "      <td>DXC Technology</td>\n",
       "      <td>3.8</td>\n",
       "      <td>(5732 Reviews)</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst - Alteryx</td>\n",
       "      <td>Capco</td>\n",
       "      <td>3.8</td>\n",
       "      <td>(163 Reviews)</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Business Analyst - Data Sciences and Ad...</td>\n",
       "      <td>Vmware</td>\n",
       "      <td>4.3</td>\n",
       "      <td>(355 Reviews)</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Business and Data Analyst</td>\n",
       "      <td>CAREERDOST ENTERPRISE</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>15,00,000 - 30,00,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst 2</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>4.3</td>\n",
       "      <td>(273 Reviews)</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job_Role  \\\n",
       "0                    Data Analyst / Business Analyst   \n",
       "1                    Data Analyst / Business Analyst   \n",
       "2  data analyst/ data analytics / Business analys...   \n",
       "3                   Business analyst + data Analysis   \n",
       "4                                       Data Analyst   \n",
       "5                Associate Professional Data Analyst   \n",
       "6                             Data Analyst - Alteryx   \n",
       "7  Senior Business Analyst - Data Sciences and Ad...   \n",
       "8                          Business and Data Analyst   \n",
       "9                                     Data Analyst 2   \n",
       "\n",
       "                         Company        Ratings   No_of_Reviews Experience  \\\n",
       "0             METRO Cash & Carry            4.2   (957 Reviews)    3-8 Yrs   \n",
       "1             METRO Cash & Carry            4.2   (957 Reviews)    3-8 Yrs   \n",
       "2  Leading US MNC into Analytics  Not available   Not available    2-7 Yrs   \n",
       "3  Anlage Infotech (I) Pvt. Ltd.  Not available   Not available   5-10 Yrs   \n",
       "4                         upGrad            3.1   (337 Reviews)    1-3 Yrs   \n",
       "5                 DXC Technology            3.8  (5732 Reviews)    3-6 Yrs   \n",
       "6                          Capco            3.8   (163 Reviews)    4-7 Yrs   \n",
       "7                         Vmware            4.3   (355 Reviews)    3-7 Yrs   \n",
       "8          CAREERDOST ENTERPRISE  Not available   Not available    0-5 Yrs   \n",
       "9                         PayPal            4.3   (273 Reviews)    1-3 Yrs   \n",
       "\n",
       "                      Salary  \\\n",
       "0  15,00,000 - 27,50,000 PA.   \n",
       "1  15,00,000 - 27,50,000 PA.   \n",
       "2  10,00,000 - 20,00,000 PA.   \n",
       "3              Not disclosed   \n",
       "4              Not disclosed   \n",
       "5              Not disclosed   \n",
       "6              Not disclosed   \n",
       "7              Not disclosed   \n",
       "8  15,00,000 - 30,00,000 PA.   \n",
       "9              Not disclosed   \n",
       "\n",
       "                                            Location  \n",
       "0                                Bangalore/Bengaluru  \n",
       "1                                Bangalore/Bengaluru  \n",
       "2  Gurgaon/Gurugram, Bangalore/Bengaluru, Delhi /...  \n",
       "3        Hyderabad/Secunderabad, Bangalore/Bengaluru  \n",
       "4                                Bangalore/Bengaluru  \n",
       "5                                Bangalore/Bengaluru  \n",
       "6                                Bangalore/Bengaluru  \n",
       "7                                Bangalore/Bengaluru  \n",
       "8                                Bangalore/Bengaluru  \n",
       "9                                Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(20)\n",
    "\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(30)\n",
    "\n",
    "#Locate the search bar\n",
    "Job_name = driver.find_element_by_class_name('suggestor-input')\n",
    "\n",
    "#Enter the search text\n",
    "Job_name.send_keys('Data Analyst')\n",
    "\n",
    "#Enter the location bar\n",
    "Location = driver.find_element_by_class_name('locationSugg')\n",
    "\n",
    "#Enter the location text\n",
    "Location.find_element_by_class_name('suggestor-input ').send_keys('Bangalore/Bengaluru')\n",
    "\n",
    "time.sleep(20)\n",
    "\n",
    "#click the crosswrapper\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "try:\n",
    "    driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
    "except NoSuchElementException as error:\n",
    "        pass\n",
    "time.sleep(60)\n",
    "\n",
    "#click the search button\n",
    "driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
    "time.sleep(30)\n",
    "\n",
    "#extract the product card\n",
    "a = driver.find_elements_by_class_name('jobTupleHeader')\n",
    "\n",
    "#From the extracted data we find that totally there are 7 datas but for some cases there were no ratings & no of ratings\n",
    "# a template which was created for all cases,\n",
    "c,Job_Role=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "    else:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "\n",
    "c,Company=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "    else:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "\n",
    "#In this case,the order changes from here on, so change the else condition as not available for not matching the if ondition\n",
    "c,Ratings=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Ratings.append(a[i].text.split('\\n')[2])\n",
    "    else:\n",
    "        Ratings.append('Not available')\n",
    "        \n",
    "#In this case,the order changes, so change the else condition as not available for not matching the if ondition\n",
    "c,No_of_Reviews=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        No_of_Reviews.append(a[i].text.split('\\n')[3])\n",
    "    else:\n",
    "        No_of_Reviews.append('Not available')\n",
    "\n",
    "#Change the order based on the previous cases\n",
    "c,Experience=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Experience.append(a[i].text.split('\\n')[4])\n",
    "    else:\n",
    "        Experience.append(a[i].text.split('\\n')[2])\n",
    "        \n",
    "c,Salary=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Salary.append(a[i].text.split('\\n')[5])\n",
    "    else:\n",
    "        Salary.append(a[i].text.split('\\n')[3])\n",
    "        \n",
    "c,Location=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Location.append(a[i].text.split('\\n')[6])\n",
    "    else:\n",
    "        Location.append(a[i].text.split('\\n')[4])\n",
    "        \n",
    "#import pandas for preparing dataframes\n",
    "import pandas as pd\n",
    "time.sleep(30)\n",
    "\n",
    "#Arrange the rows based on the category and zip it for the dataframe\n",
    "df_1 = pd.DataFrame(zip(Job_Role,Company,Ratings,No_of_Reviews,Experience,Salary,Location),columns=('Job_Role','Company','Ratings','No_of_Reviews','Experience','Salary','Location'))\n",
    "\n",
    "#Since 10 results only required, we use iloc\n",
    "df_1.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7fc081",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03de763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2900676904.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2900676904.py:11: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Job_name = driver.find_element_by_class_name('suggestor-input')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2900676904.py:17: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Location = driver.find_element_by_class_name('locationSugg')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2900676904.py:20: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Location.find_element_by_class_name('suggestor-input ').send_keys('Bangalore')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2900676904.py:23: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@title=\"Bangalore/Bengaluru\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2900676904.py:28: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2900676904.py:33: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2900676904.py:36: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  a = driver.find_elements_by_class_name('jobTupleHeader')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Company</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>No_of_Reviews</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Applied Data Scientist / ML Senior Engineer (P...</td>\n",
       "      <td>SAP India Pvt.Ltd</td>\n",
       "      <td>4.4</td>\n",
       "      <td>(1095 Reviews)</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sr. Analyst - Applied Data Scientist</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>4.1</td>\n",
       "      <td>(858 Reviews)</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist: Advanced Analytics</td>\n",
       "      <td>IBM</td>\n",
       "      <td>4.1</td>\n",
       "      <td>(13128 Reviews)</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist: Artificial Intelligence</td>\n",
       "      <td>IBM</td>\n",
       "      <td>4.1</td>\n",
       "      <td>(13128 Reviews)</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analyst - Applied Data Scientist</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>4.1</td>\n",
       "      <td>(858 Reviews)</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Research Data Scientist</td>\n",
       "      <td>Mavenir</td>\n",
       "      <td>3.9</td>\n",
       "      <td>(149 Reviews)</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lead Applied Data Scientist</td>\n",
       "      <td>Tesco Bengaluru</td>\n",
       "      <td>4.1</td>\n",
       "      <td>(858 Reviews)</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist Grade12</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(9130 Reviews)</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist Grade12</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(9130 Reviews)</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist / Analyst</td>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>4.2</td>\n",
       "      <td>(1645 Reviews)</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job_Role            Company  \\\n",
       "0  Applied Data Scientist / ML Senior Engineer (P...  SAP India Pvt.Ltd   \n",
       "1               Sr. Analyst - Applied Data Scientist              Tesco   \n",
       "2                 Data Scientist: Advanced Analytics                IBM   \n",
       "3            Data Scientist: Artificial Intelligence                IBM   \n",
       "4                   Analyst - Applied Data Scientist              Tesco   \n",
       "5                            Research Data Scientist            Mavenir   \n",
       "6                        Lead Applied Data Scientist    Tesco Bengaluru   \n",
       "7                      Senior Data Scientist Grade12           Flipkart   \n",
       "8                      Senior Data Scientist Grade12           Flipkart   \n",
       "9                           Data Scientist / Analyst      Deutsche Bank   \n",
       "\n",
       "  Ratings    No_of_Reviews Experience         Salary             Location  \n",
       "0     4.4   (1095 Reviews)   5-10 Yrs  Not disclosed  Bangalore/Bengaluru  \n",
       "1     4.1    (858 Reviews)    3-5 Yrs  Not disclosed  Bangalore/Bengaluru  \n",
       "2     4.1  (13128 Reviews)    2-5 Yrs  Not disclosed  Bangalore/Bengaluru  \n",
       "3     4.1  (13128 Reviews)    3-7 Yrs  Not disclosed  Bangalore/Bengaluru  \n",
       "4     4.1    (858 Reviews)    1-2 Yrs  Not disclosed  Bangalore/Bengaluru  \n",
       "5     3.9    (149 Reviews)    4-9 Yrs  Not disclosed  Bangalore/Bengaluru  \n",
       "6     4.1    (858 Reviews)    4-7 Yrs  Not disclosed  Bangalore/Bengaluru  \n",
       "7     4.2   (9130 Reviews)   5-10 Yrs  Not disclosed  Bangalore/Bengaluru  \n",
       "8     4.2   (9130 Reviews)    5-7 Yrs  Not disclosed  Bangalore/Bengaluru  \n",
       "9     4.2   (1645 Reviews)    2-6 Yrs  Not disclosed  Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#Locate the search bar\n",
    "Job_name = driver.find_element_by_class_name('suggestor-input')\n",
    "\n",
    "#Enter the search text\n",
    "Job_name.send_keys('Data Scientist')\n",
    "\n",
    "#Enter the location bar\n",
    "Location = driver.find_element_by_class_name('locationSugg')\n",
    "\n",
    "#Enter the location text\n",
    "Location.find_element_by_class_name('suggestor-input ').send_keys('Bangalore')\n",
    "time.sleep(30)\n",
    "#Click on the Bangalore icon on the drop down\n",
    "driver.find_element_by_xpath('//div[@title=\"Bangalore/Bengaluru\"]').click()\n",
    "time.sleep(30)\n",
    "#click the crosswrapper\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "try:\n",
    "    driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
    "except NoSuchElementException as error:\n",
    "        pass\n",
    "time.sleep(30)    \n",
    "#click the search button\n",
    "driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
    "time.sleep(30)\n",
    "#extract the product card\n",
    "a = driver.find_elements_by_class_name('jobTupleHeader')\n",
    "\n",
    "#From the extracted data we find that totally there are 7 datas but for some cases there were no ratings & no of ratings\n",
    "# a template which was created for all cases,\n",
    "c,Job_Role=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "    else:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "\n",
    "\n",
    "c,Company=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "    else:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "\n",
    "#In this case,the order changes from here on, so change the else condition as not available for not matching the if ondition\n",
    "c,Ratings=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Ratings.append(a[i].text.split('\\n')[2])\n",
    "    else:\n",
    "        Ratings.append('Not available')\n",
    "        \n",
    "#In this case,the order changes, so change the else condition as not available for not matching the if ondition\n",
    "c,No_of_Reviews=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        No_of_Reviews.append(a[i].text.split('\\n')[3])\n",
    "    else:\n",
    "        No_of_Reviews.append('Not available')\n",
    "        \n",
    "#Change the order based on the previous cases\n",
    "c,Experience=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Experience.append(a[i].text.split('\\n')[4])\n",
    "    else:\n",
    "        Experience.append(a[i].text.split('\\n')[2])\n",
    "        \n",
    "c,Salary=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Salary.append(a[i].text.split('\\n')[5])\n",
    "    else:\n",
    "        Salary.append(a[i].text.split('\\n')[3])\n",
    "        \n",
    "c,Location=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Location.append(a[i].text.split('\\n')[6])\n",
    "    else:\n",
    "        Location.append(a[i].text.split('\\n')[4])\n",
    "        \n",
    "#import pandas for preparing dataframes\n",
    "import pandas as pd\n",
    "\n",
    "#Arrange the rows based on the category and zip it for the dataframe\n",
    "df_2 = pd.DataFrame(zip(Job_Role,Company,Ratings,No_of_Reviews,Experience,Salary,Location),columns=('Job_Role','Company','Ratings','No_of_Reviews','Experience','Salary','Location'))\n",
    "\n",
    "#Since 10 results only required, we use iloc\n",
    "df_2.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb923a9",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below: You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs The task will be done as shown in the below steps:\n",
    "\n",
    "first get the webpage https://www.naukri.com/\n",
    "Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "Then click the search button.\n",
    "Then apply the location filter and salary filter by checking the respective boxes\n",
    "Then scrape the data for the first 10 jobs results you get.\n",
    "Finally create a dataframe of the scraped data. Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f23494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/1590294169.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/1590294169.py:11: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Job_name = driver.find_element_by_class_name('suggestor-input')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/1590294169.py:19: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/1590294169.py:24: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/1590294169.py:28: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath(\".//*[contains(text(), 'Delhi / NCR')]\").click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/1590294169.py:32: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath(\".//*[contains(text(), '3-6 Lakhs')]\").click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/1590294169.py:35: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  a = driver.find_elements_by_class_name('jobTupleHeader')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Company</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>No_of_Reviews</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Internet Jobs - II</td>\n",
       "      <td>Jobs Territory</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>3.7</td>\n",
       "      <td>(22 Reviews)</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>EASY DATA ANALYTICS TECHNOLOGY PRIVATE LIMITED</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "      <td>4,00,000 - 6,00,000 PA.</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Associate Scientist - Data Engineering</td>\n",
       "      <td>AXA Technology Services India Pvt. Ltd</td>\n",
       "      <td>4.9</td>\n",
       "      <td>(3 Reviews)</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist || Software Company || Immediat...</td>\n",
       "      <td>Skyleaf Consultants</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist (freelance)</td>\n",
       "      <td>2Coms</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>4,75,000 - 9,75,000 PA.</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist/ Machine Learning, 2022 Passout...</td>\n",
       "      <td>Creative Hands HR Consultancy</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "      <td>5,00,000 - 7,00,000 PA.</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Delhi /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>BlackBuck</td>\n",
       "      <td>4.0</td>\n",
       "      <td>(929 Reviews)</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon, Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Associate Data Scientist</td>\n",
       "      <td>Optum</td>\n",
       "      <td>4.1</td>\n",
       "      <td>(1549 Reviews)</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hiring For Data Analyst and Data Scientist For...</td>\n",
       "      <td>Shadow Placements</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>5,00,000 - 13,00,000 PA.</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job_Role  \\\n",
       "0                Data Scientist - Internet Jobs - II   \n",
       "1                                     Data Scientist   \n",
       "2                              Junior Data Scientist   \n",
       "3             Associate Scientist - Data Engineering   \n",
       "4  Data Scientist || Software Company || Immediat...   \n",
       "5                         Data Scientist (freelance)   \n",
       "6  Data Scientist/ Machine Learning, 2022 Passout...   \n",
       "7                                     Data Scientist   \n",
       "8                           Associate Data Scientist   \n",
       "9  Hiring For Data Analyst and Data Scientist For...   \n",
       "\n",
       "                                          Company        Ratings  \\\n",
       "0                                  Jobs Territory  Not available   \n",
       "1              Ashkom Media India Private Limited            3.7   \n",
       "2  EASY DATA ANALYTICS TECHNOLOGY PRIVATE LIMITED  Not available   \n",
       "3          AXA Technology Services India Pvt. Ltd            4.9   \n",
       "4                             Skyleaf Consultants  Not available   \n",
       "5                                           2Coms  Not available   \n",
       "6                   Creative Hands HR Consultancy  Not available   \n",
       "7                                       BlackBuck            4.0   \n",
       "8                                           Optum            4.1   \n",
       "9                               Shadow Placements  Not available   \n",
       "\n",
       "    No_of_Reviews Experience                    Salary  \\\n",
       "0   Not available    3-6 Yrs             Not disclosed   \n",
       "1    (22 Reviews)    3-6 Yrs             Not disclosed   \n",
       "2   Not available    1-2 Yrs   4,00,000 - 6,00,000 PA.   \n",
       "3     (3 Reviews)    2-5 Yrs             Not disclosed   \n",
       "4   Not available    3-8 Yrs             Not disclosed   \n",
       "5   Not available    2-7 Yrs   4,75,000 - 9,75,000 PA.   \n",
       "6   Not available    0-4 Yrs   5,00,000 - 7,00,000 PA.   \n",
       "7   (929 Reviews)    3-7 Yrs             Not disclosed   \n",
       "8  (1549 Reviews)    1-5 Yrs             Not disclosed   \n",
       "9   Not available    3-7 Yrs  5,00,000 - 13,00,000 PA.   \n",
       "\n",
       "                                            Location  \n",
       "0  Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...  \n",
       "1                         Noida, Bangalore/Bengaluru  \n",
       "2                                              Noida  \n",
       "3                                   Gurgaon/Gurugram  \n",
       "4              Gurgaon/Gurugram, Bangalore/Bengaluru  \n",
       "5                                   New Delhi, Delhi  \n",
       "6  Hyderabad/Secunderabad, Pune, Chennai, Delhi /...  \n",
       "7                                 Gurgaon, Bengaluru  \n",
       "8                                   Gurgaon/Gurugram  \n",
       "9               Noida, Gurgaon/Gurugram, Delhi / NCR  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(10)\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(10)\n",
    "#Locate the search bar\n",
    "Job_name = driver.find_element_by_class_name('suggestor-input')\n",
    "\n",
    "#Enter the search text\n",
    "Job_name.send_keys('Data Scientist')\n",
    "\n",
    "#click the crosswrapper\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "try:\n",
    "    driver.find_element_by_xpath('//div[@class=\"app-icon app-icon-cross crossIcon\"]').click()\n",
    "except NoSuchElementException as error:\n",
    "        pass\n",
    "time.sleep(30)    \n",
    "#click the search button\n",
    "driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]').click()\n",
    "time.sleep(30)\n",
    "\n",
    "#Click on the bangalore dropdown using xpath\n",
    "driver.find_element_by_xpath(\".//*[contains(text(), 'Delhi / NCR')]\").click()\n",
    "time.sleep(30)\n",
    "\n",
    "#Click on the bangalore dropdown using xpath\n",
    "driver.find_element_by_xpath(\".//*[contains(text(), '3-6 Lakhs')]\").click()\n",
    "time.sleep(30)\n",
    "#extract the product card\n",
    "a = driver.find_elements_by_class_name('jobTupleHeader')\n",
    "\n",
    "#From the extracted data we find that totally there are 7 datas but for some cases there were no ratings & no of ratings\n",
    "# a template which was created for all cases,\n",
    "c,Job_Role=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "    else:\n",
    "        Job_Role.append(a[i].text.split('\\n')[0])\n",
    "        \n",
    "c,Company=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "    else:\n",
    "        Company.append(a[i].text.split('\\n')[1])\n",
    "\n",
    "#In this case,the order changes from here on, so change the else condition as not available for not matching the if ondition\n",
    "c,Ratings=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Ratings.append(a[i].text.split('\\n')[2])\n",
    "    else:\n",
    "        Ratings.append('Not available')\n",
    "        \n",
    "#In this case,the order changes, so change the else condition as not available for not matching the if ondition\n",
    "c,No_of_Reviews=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        No_of_Reviews.append(a[i].text.split('\\n')[3])\n",
    "    else:\n",
    "        No_of_Reviews.append('Not available')\n",
    "        \n",
    "#Change the order based on the previous cases\n",
    "c,Experience=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Experience.append(a[i].text.split('\\n')[4])\n",
    "    else:\n",
    "        Experience.append(a[i].text.split('\\n')[2])\n",
    "        \n",
    "c,Salary=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Salary.append(a[i].text.split('\\n')[5])\n",
    "    else:\n",
    "        Salary.append(a[i].text.split('\\n')[3])\n",
    "        \n",
    "c,Location=[],[]\n",
    "for i in a:\n",
    "    c.append(len(i.text.split('\\n')))\n",
    "for i in range(20):\n",
    "    if c[i]==7:\n",
    "        Location.append(a[i].text.split('\\n')[6])\n",
    "    else:\n",
    "        Location.append(a[i].text.split('\\n')[4])\n",
    "        \n",
    "#import pandas for preparing dataframes\n",
    "import pandas as pd\n",
    "\n",
    "#Arrange the rows based on the category and zip it for the dataframe\n",
    "df_3 = pd.DataFrame(zip(Job_Role,Company,Ratings,No_of_Reviews,Experience,Salary,Location),columns=('Job_Role','Company','Ratings','No_of_Reviews','Experience','Salary','Location'))\n",
    "\n",
    "#Since 10 results only required, we use iloc\n",
    "df_3.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d6ed4",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "Brand\n",
    "Product Description\n",
    "Price The attributes which you have to scrape is ticked marked in the below image. To scrape the data you have to go through following steps:\n",
    "Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "Enter “sunglasses” in the search field where “search for products, brands andmore” is written and click the search icon\n",
    "After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the required data as usual. ASSIGNMENT 2\n",
    "After scraping data from the first page, go to the “Next” Button at the bottom ofthe page , then click on it.\n",
    "Now scrape data from this page as usual\n",
    "Repeat this until you get data for 100 sunglasses. Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4f5b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/789708475.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/789708475.py:11: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  Login = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/789708475.py:14: DeprecationWarning: find_element_by_name is deprecated. Please use find_element(by=By.NAME, value=name) instead\n",
      "  driver.find_element_by_name('q').send_keys('sun glasses')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/789708475.py:17: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/789708475.py:20: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  page = driver.find_elements_by_class_name('ge-49M')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/789708475.py:37: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/789708475.py:39: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('_2B099V'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/789708475.py:61: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/789708475.py:61: DeprecationWarning: find_element_by_tag_name is deprecated. Please use find_element(by=By.TAG_NAME, value=name) instead\n",
      "  Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_Brand</th>\n",
       "      <th>Product_Price</th>\n",
       "      <th>Product_Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UV Protection Cat-eye Sunglasses (54)</td>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>₹599</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UV Protection Rectangular Sunglasses (50)</td>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>₹599</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>₹669</td>\n",
       "      <td>25% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UV Protection, Polarized Wayfarer Sunglasses (...</td>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>₹233</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>₹589</td>\n",
       "      <td>26% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>by Lenskart Polarized, UV Protection Wayfarer ...</td>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>₹679</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>UV Protection Oval Sunglasses (57)</td>\n",
       "      <td>Roadster</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>UV Protection, Mirrored Sports Sunglasses (62)</td>\n",
       "      <td>NuVew</td>\n",
       "      <td>₹295</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>Poloport</td>\n",
       "      <td>₹299</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>UV Protection Retro Square, Wayfarer Sunglasse...</td>\n",
       "      <td>NuVew</td>\n",
       "      <td>₹225</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Product_Name  Product_Brand  \\\n",
       "0                UV Protection Cat-eye Sunglasses (54)  VINCENT CHASE   \n",
       "1            UV Protection Rectangular Sunglasses (50)  VINCENT CHASE   \n",
       "2        UV Protection Wayfarer Sunglasses (Free Size)       Fastrack   \n",
       "3    UV Protection, Polarized Wayfarer Sunglasses (...         SUNBEE   \n",
       "4     UV Protection Rectangular Sunglasses (Free Size)       Fastrack   \n",
       "..                                                 ...            ...   \n",
       "115  by Lenskart Polarized, UV Protection Wayfarer ...  VINCENT CHASE   \n",
       "116                 UV Protection Oval Sunglasses (57)       Roadster   \n",
       "117     UV Protection, Mirrored Sports Sunglasses (62)          NuVew   \n",
       "118      UV Protection Wayfarer Sunglasses (Free Size)       Poloport   \n",
       "119  UV Protection Retro Square, Wayfarer Sunglasse...          NuVew   \n",
       "\n",
       "    Product_Price Product_Discount  \n",
       "0            ₹599          70% off  \n",
       "1            ₹599          70% off  \n",
       "2            ₹669          25% off  \n",
       "3            ₹233          82% off  \n",
       "4            ₹589          26% off  \n",
       "..            ...              ...  \n",
       "115          ₹679          72% off  \n",
       "116          ₹599          40% off  \n",
       "117          ₹295          76% off  \n",
       "118          ₹299          83% off  \n",
       "119          ₹225          78% off  \n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(10)\n",
    "#click on the login close page\n",
    "Login = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "time.sleep(10)\n",
    "#locate the search bar give input\n",
    "driver.find_element_by_name('q').send_keys('sun glasses')\n",
    "\n",
    "#click on the search button\n",
    "driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
    "time.sleep(10)\n",
    "#extract  the page\n",
    "page = driver.find_elements_by_class_name('ge-49M')\n",
    "\n",
    "#extract the first ten pages\n",
    "link = []\n",
    "for i in page:\n",
    "    link.append(i.get_attribute('href'))\n",
    "time.sleep(10)    \n",
    "#since we need 100 results we need only first 3 pages\n",
    "a = link[0:3]\n",
    "time.sleep(10)\n",
    "#Extract the first three pages & its product data\n",
    "product = []\n",
    "for el in a:\n",
    "    #import selenium,webdriver,upload webdriver.exe\n",
    "    from selenium import webdriver as wb\n",
    "    import time\n",
    "    from selenium.webdriver.support.select import Select\n",
    "    driver = wb.Chrome('chromedriver.exe')\n",
    "    driver.get(el)\n",
    "    for i in driver.find_elements_by_class_name('_2B099V'):\n",
    "        product.append(i)\n",
    "        \n",
    "#Extract Product_brand by its class_name\n",
    "Product_Brand = []\n",
    "for i in product:\n",
    "    Product_Brand.append(i.find_elements_by_class_name('_2WkVRV')[0].text)\n",
    "\n",
    "#Extract Product_Name by its class_name\n",
    "Product_Name = []\n",
    "for i in product:\n",
    "    Product_Name.append(i.find_elements_by_class_name('IRpwTa')[0].get_attribute('title'))\n",
    "    \n",
    "#Extract Product_Name by its class_name\n",
    "Product_Price = []\n",
    "for i in product:\n",
    "    Product_Price.append(i.find_elements_by_class_name('_30jeq3')[0].text)\n",
    "\n",
    "\n",
    "#Extract Product_Name by its class_name\n",
    "Product_Discount = []\n",
    "for i in product:\n",
    "    Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n",
    "    \n",
    "# Import pandas to create pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Extract dataFrame\n",
    "df_4 = pd.DataFrame(zip(Product_Name,Product_Brand,Product_Price,Product_Discount),columns=(('Product_Name','Product_Brand','Product_Price','Product_Discount')))\n",
    "\n",
    "#Extracted data for first 100 sunglasess\n",
    "df_4.iloc[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec4f6e",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace. When you will open the above link you will reach to the below shown webpage . As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "\n",
    "Rating\n",
    "Review summary\n",
    "Full review\n",
    "You have to scrape this data for first 100 reviews. Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5623db94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2335143536.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2335143536.py:11: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('_3TOw5k')[0].find_elements_by_tag_name('a')[14].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2335143536.py:11: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('_3TOw5k')[0].find_elements_by_tag_name('a')[14].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2335143536.py:15: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('ge-49M'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2335143536.py:24: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2335143536.py:26: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('_27M-vq'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2335143536.py:32: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  rating.append(i.find_element_by_class_name('row').text.split('\\n'))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2335143536.py:40: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Review_description.append(i.find_element_by_class_name('t-ZTKy').text.replace('\\n',''))\n"
     ]
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(10)\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC')\n",
    "time.sleep(10)\n",
    "#find the next page url and click\n",
    "driver.find_elements_by_class_name('_3TOw5k')[0].find_elements_by_tag_name('a')[14].click()\n",
    "time.sleep(10)\n",
    "#Extract the first 10 pages\n",
    "nex = []\n",
    "for i in driver.find_elements_by_class_name('ge-49M'):\n",
    "    nex.append(i.get_attribute('href'))\n",
    "time.sleep(10)   \n",
    "#extract data from 10 pages and store it in review\n",
    "review = []\n",
    "for el in nex:\n",
    "    from selenium import webdriver as wb\n",
    "    import time\n",
    "    from selenium.webdriver.support.select import Select\n",
    "    driver = wb.Chrome('chromedriver.exe')\n",
    "    driver.get(el)\n",
    "    for i in driver.find_elements_by_class_name('_27M-vq'):\n",
    "        review.append(i)\n",
    "time.sleep(10)       \n",
    "# extract rating(star & header) \n",
    "rating = []\n",
    "for i in review:\n",
    "    rating.append(i.find_element_by_class_name('row').text.split('\\n'))\n",
    "    \n",
    "#seperate the star & heading\n",
    "review_star,review_heading = (list(zip(*rating)))\n",
    "\n",
    "# extract description using the class name\n",
    "Review_description = []\n",
    "for i in review:\n",
    "    Review_description.append(i.find_element_by_class_name('t-ZTKy').text.replace('\\n',''))\n",
    "    \n",
    "#import pandas for preapring dataframe\n",
    "import pandas as pd\n",
    "\n",
    "#inally zip with proper titles\n",
    "\n",
    "df_5 = pd.DataFrame(zip(review_star,review_heading,Review_description),columns=(('Review_star','Review_heading','Review_description')))                  \n",
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94dc0394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_star</th>\n",
       "      <th>Review_heading</th>\n",
       "      <th>Review_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the MoneyThe iPhone 11 offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>A perfect phone and a good battery super camer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>This is my first ever iPhone.And I truly don’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Undoubtedly Iphone 11 is the most successful m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>I purchased the iPhone 11 a month back. I must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>Nice value for money good and best price I pho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Review_star       Review_heading  \\\n",
       "0            5            Brilliant   \n",
       "1            5       Simply awesome   \n",
       "2            5  Best in the market!   \n",
       "3            5     Perfect product!   \n",
       "4            5            Fabulous!   \n",
       "..         ...                  ...   \n",
       "95           5            Excellent   \n",
       "96           5               Super!   \n",
       "97           5    Worth every penny   \n",
       "98           5            Fabulous!   \n",
       "99           5            Wonderful   \n",
       "\n",
       "                                   Review_description  \n",
       "0   The Best Phone for the MoneyThe iPhone 11 offe...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  A perfect phone and a good battery super camer...  \n",
       "96  This is my first ever iPhone.And I truly don’t...  \n",
       "97  Undoubtedly Iphone 11 is the most successful m...  \n",
       "98  I purchased the iPhone 11 a month back. I must...  \n",
       "99  Nice value for money good and best price I pho...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558dfbf",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for “sneakers” in the search field. You have to scrape 4 attributes of each sneaker:\n",
    "\n",
    "Brand\n",
    "Product Description\n",
    "Price As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a98b3830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/594219936.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/594219936.py:11: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  Login = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/594219936.py:14: DeprecationWarning: find_element_by_name is deprecated. Please use find_element(by=By.NAME, value=name) instead\n",
      "  driver.find_element_by_name('q').send_keys('sneakers')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/594219936.py:17: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/594219936.py:20: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  page = driver.find_elements_by_class_name('ge-49M')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/594219936.py:37: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/594219936.py:39: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('_2B099V'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/594219936.py:60: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/594219936.py:60: DeprecationWarning: find_element_by_tag_name is deprecated. Please use find_element(by=By.TAG_NAME, value=name) instead\n",
      "  Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_Brand</th>\n",
       "      <th>Product_Price</th>\n",
       "      <th>Product_Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Casual Sneaker Shoes Sneakers For Men</td>\n",
       "      <td>Figor</td>\n",
       "      <td>₹599</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Luxury Fashionable Breathable Casual Sneakers ...</td>\n",
       "      <td>HOC</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sneakers Sneakers For Men</td>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>₹148</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>BRUTON</td>\n",
       "      <td>₹245</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kwik FIT casual sneaker shoes and partywear sh...</td>\n",
       "      <td>KWIK FIT</td>\n",
       "      <td>₹349</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>aadi</td>\n",
       "      <td>₹321</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>STRANGER BROTHERS</td>\n",
       "      <td>₹359</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>casual shoe for men Sneakers For Men</td>\n",
       "      <td>bluemaker</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Fashion Outdoor Canvas Casual Light Weight Lac...</td>\n",
       "      <td>RODDICK SHOES</td>\n",
       "      <td>₹380</td>\n",
       "      <td>61% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NEW DENIM BLACK STRIP Sneakers For Men</td>\n",
       "      <td>AROOTER</td>\n",
       "      <td>₹421</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Product_Name      Product_Brand  \\\n",
       "0               Casual Sneaker Shoes Sneakers For Men              Figor   \n",
       "1   Luxury Fashionable Breathable Casual Sneakers ...                HOC   \n",
       "2                           Sneakers Sneakers For Men           URBANBOX   \n",
       "3       Modern Trendy Sneakers Shoes Sneakers For Men             BRUTON   \n",
       "4   Kwik FIT casual sneaker shoes and partywear sh...           KWIK FIT   \n",
       "..                                                ...                ...   \n",
       "95                                   Sneakers For Men               aadi   \n",
       "96                                   Sneakers For Men  STRANGER BROTHERS   \n",
       "97               casual shoe for men Sneakers For Men          bluemaker   \n",
       "98  Fashion Outdoor Canvas Casual Light Weight Lac...      RODDICK SHOES   \n",
       "99             NEW DENIM BLACK STRIP Sneakers For Men            AROOTER   \n",
       "\n",
       "   Product_Price Product_Discount  \n",
       "0           ₹599          60% off  \n",
       "1           ₹399          80% off  \n",
       "2           ₹148          85% off  \n",
       "3           ₹245          81% off  \n",
       "4           ₹349          82% off  \n",
       "..           ...              ...  \n",
       "95          ₹321          83% off  \n",
       "96          ₹359          64% off  \n",
       "97          ₹399          60% off  \n",
       "98          ₹380          61% off  \n",
       "99          ₹421          73% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(10)\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(10)\n",
    "#click on the login close page\n",
    "Login = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "time.sleep(10)\n",
    "#locate the search bar give input\n",
    "driver.find_element_by_name('q').send_keys('sneakers')\n",
    "time.sleep(10)\n",
    "#click on the search button\n",
    "driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
    "time.sleep(10)\n",
    "#extract  the page\n",
    "page = driver.find_elements_by_class_name('ge-49M')\n",
    "time.sleep(10)\n",
    "#extract the first ten pages\n",
    "link = []\n",
    "for i in page:\n",
    "    link.append(i.get_attribute('href'))\n",
    "time.sleep(10)    \n",
    "#since we need 100 results we need only first 3 pages\n",
    "a = link[0:3]\n",
    "time.sleep(10)\n",
    "#Extract the first three pages & its product data\n",
    "product = []\n",
    "for el in a:\n",
    "    #import selenium,webdriver,upload webdriver.exe\n",
    "    from selenium import webdriver as wb\n",
    "    import time\n",
    "    from selenium.webdriver.support.select import Select\n",
    "    driver = wb.Chrome('chromedriver.exe')\n",
    "    driver.get(el)\n",
    "    for i in driver.find_elements_by_class_name('_2B099V'):\n",
    "        product.append(i)\n",
    "time.sleep(10)        \n",
    "#Extract Product_brand by its class_name\n",
    "Product_Brand = []\n",
    "for i in product:\n",
    "    Product_Brand.append(i.find_elements_by_class_name('_2WkVRV')[0].text)\n",
    "    \n",
    "#Extract Product_Name by its class_name\n",
    "Product_Name = []\n",
    "for i in product:\n",
    "    Product_Name.append(i.find_elements_by_class_name('IRpwTa')[0].get_attribute('title'))\n",
    "    \n",
    "#Extract Product_Name by its class_name\n",
    "Product_Price = []\n",
    "for i in product:\n",
    "    Product_Price.append(i.find_elements_by_class_name('_30jeq3')[0].text)\n",
    "    \n",
    "#Extract Product_Name by its class_name\n",
    "Product_Discount = []\n",
    "for i in product:\n",
    "    Product_Discount.append(i.find_element_by_class_name('_25b18c').find_element_by_tag_name('span').text)\n",
    "    \n",
    "# Import pandas to create pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Extract dataFrame\n",
    "df_6 = pd.DataFrame(zip(Product_Name,Product_Brand,Product_Price,Product_Discount),columns=(('Product_Name','Product_Brand','Product_Price','Product_Discount')))\n",
    "\n",
    "#Extracted data for first 100 sunglasess\n",
    "df_6.iloc[0:100:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c8991",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 7149 to Rs. 14099 ” , Color filter to “Black”, as shown inthe below image. And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18159a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:11: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//li[@class=\"colour-listItem\"]').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:14: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_element_by_class_name('price-list').find_elements_by_tag_name('li')[1].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:14: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  driver.find_element_by_class_name('price-list').find_elements_by_tag_name('li')[1].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:17: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('product-productMetaInfo')[0].text\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:20: DeprecationWarning: find_element_by_link_text is deprecated. Please use find_element(by=By.LINK_TEXT, value=link_text) instead\n",
      "  page = driver.find_element_by_link_text('2').get_attribute('href')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:36: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:40: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  for i in driver.find_elements_by_class_name('product-base'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:46: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Product_Name.append(a[i].find_element_by_class_name('product-product').text.split('\\n')[0])\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2249798817.py:51: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  Product_Brand.append(a[i].find_element_by_class_name('product-brand').text.split('\\n')[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_Brand</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Men React Miler2 Running Shoes</td>\n",
       "      <td>Nike</td>\n",
       "      <td>11495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Men Charged Rouge 3 Run Shoes</td>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Men Sneakers</td>\n",
       "      <td>ALDO</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Men Max Cushioning Running</td>\n",
       "      <td>Skechers</td>\n",
       "      <td>7199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women React MR 3 Running Shoes</td>\n",
       "      <td>Nike</td>\n",
       "      <td>8920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Men Formal Leather Derby</td>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>7990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Men Leather Formal Loafers</td>\n",
       "      <td>J.FONTINI</td>\n",
       "      <td>7490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Block Pumps with Bows</td>\n",
       "      <td>ALDO</td>\n",
       "      <td>8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Women Leather Loafers</td>\n",
       "      <td>Cole Haan</td>\n",
       "      <td>11899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Women Textured Mules</td>\n",
       "      <td>Bugatti</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Product_Name         Product_Brand    Rate\n",
       "0   Men React Miler2 Running Shoes                  Nike   11495\n",
       "1    Men Charged Rouge 3 Run Shoes          UNDER ARMOUR    7999\n",
       "2                     Men Sneakers                  ALDO    9999\n",
       "3       Men Max Cushioning Running              Skechers    7199\n",
       "4   Women React MR 3 Running Shoes                  Nike    8920\n",
       "..                             ...                   ...     ...\n",
       "95        Men Formal Leather Derby  Heel & Buckle London    7990\n",
       "96      Men Leather Formal Loafers             J.FONTINI    7490\n",
       "97           Block Pumps with Bows                  ALDO    8999\n",
       "98           Women Leather Loafers             Cole Haan   11899\n",
       "99            Women Textured Mules               Bugatti    7999\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium,webdriver,upload webdriver.exe\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "time.sleep(10)\n",
    "#get the url and extract into html\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.myntra.com/shoes')\n",
    "time.sleep(10)\n",
    "#Select the colour checklist and click\n",
    "driver.find_element_by_xpath('//li[@class=\"colour-listItem\"]').click()\n",
    "time.sleep(10)\n",
    "#Select the price checklist and click\n",
    "driver.find_element_by_class_name('price-list').find_elements_by_tag_name('li')[1].click()\n",
    "time.sleep(10)\n",
    "#extract the product data\n",
    "driver.find_elements_by_class_name('product-productMetaInfo')[0].text\n",
    "time.sleep(10)\n",
    "#extract page url\n",
    "page = driver.find_element_by_link_text('2').get_attribute('href')\n",
    "\n",
    "time.sleep(10)#extract all pages\n",
    "li = []\n",
    "for i in range(1,19):\n",
    "    li.append(page[:-1]+str(i))\n",
    "time.sleep(10)    \n",
    "#As we need 100 products,we extract only 3 pages\n",
    "k =li[0:2]\n",
    "time.sleep(10)\n",
    "#extract data from 3 pages\n",
    "a = []\n",
    "for el in k:\n",
    "    from selenium import webdriver\n",
    "    import time\n",
    "\n",
    "    driver=webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(el)\n",
    "\n",
    "\n",
    "    for i in driver.find_elements_by_class_name('product-base'):\n",
    "        a.append(i)\n",
    "time.sleep(10)        \n",
    "#Extract product name info\n",
    "Product_Name=[]\n",
    "for i in range(100):\n",
    "    Product_Name.append(a[i].find_element_by_class_name('product-product').text.split('\\n')[0])\n",
    "    \n",
    "#Extract product brand info\n",
    "Product_Brand=[]\n",
    "for i in range(100):\n",
    "    Product_Brand.append(a[i].find_element_by_class_name('product-brand').text.split('\\n')[0])\n",
    "\n",
    "#Extract product price info\n",
    "Product_Price=[]\n",
    "for i in range(100):\n",
    "    Product_Price.append(a[i].find_elements_by_class_name('product-price')[0].text)\n",
    "    #dataframe for 100 products\n",
    "    \n",
    "    \n",
    "#Extract product rate info\n",
    "Rate = []\n",
    "for i in range(100):\n",
    "    Rate.append(Product_Price[i].split('Rs.')[1])\n",
    "    \n",
    "#import pandas for preparing dataframe\n",
    "import pandas as pd\n",
    "df_7 = pd.DataFrame(zip(Product_Name,Product_Brand,Rate),columns=('Product_Name','Product_Brand','Rate'))\n",
    "\n",
    "#only for 100 products so we use iloc\n",
    "df_7.iloc[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab35081f",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fda81948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/49110030.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/49110030.py:11: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('twotabsearchtextbox').send_keys('Laptop')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/49110030.py:14: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('nav-search-submit-button').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/49110030.py:17: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('p_n_feature_thirteen_browse-bin/12598163031').find_element_by_tag_name('span').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/49110030.py:17: DeprecationWarning: find_element_by_tag_name is deprecated. Please use find_element(by=By.TAG_NAME, value=name) instead\n",
      "  driver.find_element_by_id('p_n_feature_thirteen_browse-bin/12598163031').find_element_by_tag_name('span').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/49110030.py:20: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  d = driver.find_elements_by_xpath('//div[@class=\"a-section a-spacing-small a-spacing-top-small\"]')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/49110030.py:39: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  name.append(d[i].find_element_by_xpath('*//span[@class=\"a-size-medium a-color-base a-text-normal\"]').text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/49110030.py:48: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  price.append(d[i].find_element_by_class_name('a-price').text)\n"
     ]
    }
   ],
   "source": [
    "#import selenium wedriver and time\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#use chromedriver\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.amazon.in/')\n",
    "time.sleep(10)\n",
    "#give input\n",
    "driver.find_element_by_id('twotabsearchtextbox').send_keys('Laptop')\n",
    "time.sleep(10)\n",
    "#click on search button\n",
    "driver.find_element_by_id('nav-search-submit-button').click()\n",
    "time.sleep(10)\n",
    "#click on i7 checkbox button\n",
    "driver.find_element_by_id('p_n_feature_thirteen_browse-bin/12598163031').find_element_by_tag_name('span').click()\n",
    "time.sleep(10)\n",
    "#extract product data\n",
    "d = driver.find_elements_by_xpath('//div[@class=\"a-section a-spacing-small a-spacing-top-small\"]')\n",
    "time.sleep(10)                                  \n",
    "\n",
    "#extract product ratings,use exception due to unvavailabilty of data\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "ratings = []\n",
    "for i in range(31):\n",
    "    try:\n",
    "        ratings.append(d[i].find_elements_by_class_name('a-icon-alt')[0].get_attribute('innerHTML'))\n",
    "    except (IndexError,NoSuchElementException) as error:\n",
    "        ratings.append('Not available')\n",
    "\n",
    "\n",
    "        \n",
    "#extract product name,use exception due to unvavailabilty of data\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "name = []\n",
    "for i in range(31):\n",
    "    try:\n",
    "        name.append(d[i].find_element_by_xpath('*//span[@class=\"a-size-medium a-color-base a-text-normal\"]').text)\n",
    "    except (IndexError,NoSuchElementException) as error:\n",
    "        name.append('Not available')\n",
    "        \n",
    "#extract product price,use exception due to unvavailabilty of data\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "price = []\n",
    "for i in range(31):\n",
    "    try:\n",
    "        price.append(d[i].find_element_by_class_name('a-price').text)\n",
    "    except (IndexError,NoSuchElementException) as error:\n",
    "        price.append('Not available')\n",
    "        \n",
    "#extract product ratings,use exception due to unvavailabilty of data\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "ratings = []\n",
    "for i in range(31):\n",
    "    try:\n",
    "        ratings.append(d[i].find_elements_by_class_name('a-icon-alt')[0].get_attribute('innerHTML'))\n",
    "    except (IndexError,NoSuchElementException) as error:\n",
    "        ratings.append('Not available')\n",
    "                                  \n",
    "#import pandas and extract data frame for 10 products\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(zip(name,ratings,price),columns = ('Product_Name','Product_Ratings','Product_Price'))\n",
    "df_8 = df.iloc[1:11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f361ac4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_Ratings</th>\n",
       "      <th>Product_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI Modern 14, Intel i7-1195G7, 14\" FHD IPS-Le...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>₹76,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samsung Galaxy Book2 Pro 360 Intel 12th Gen i7...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>₹1,29,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mi Notebook Ultra 3.2K Resolution Display Inte...</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>₹77,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>₹86,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>₹57,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>₹89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HP Pavilion 15 12th Gen Intel Core i7 16GB SDR...</td>\n",
       "      <td>Not available</td>\n",
       "      <td>₹92,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Samsung Galaxy Book2 Intel 12th Gen core i7 39...</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "      <td>₹79,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HP Pavilion x360 11th Gen Intel Core i7 14 inc...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>₹85,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Acer Predator Helios 300 11th Gen Intel Core i...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>₹1,69,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Product_Name     Product_Ratings  \\\n",
       "1   MSI Modern 14, Intel i7-1195G7, 14\" FHD IPS-Le...  4.0 out of 5 stars   \n",
       "2   Samsung Galaxy Book2 Pro 360 Intel 12th Gen i7...  5.0 out of 5 stars   \n",
       "3   Mi Notebook Ultra 3.2K Resolution Display Inte...  4.3 out of 5 stars   \n",
       "4   Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....  4.4 out of 5 stars   \n",
       "5   ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...  4.4 out of 5 stars   \n",
       "6   ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...  4.6 out of 5 stars   \n",
       "7   HP Pavilion 15 12th Gen Intel Core i7 16GB SDR...       Not available   \n",
       "8   Samsung Galaxy Book2 Intel 12th Gen core i7 39...  3.0 out of 5 stars   \n",
       "9   HP Pavilion x360 11th Gen Intel Core i7 14 inc...  4.0 out of 5 stars   \n",
       "10  Acer Predator Helios 300 11th Gen Intel Core i...  5.0 out of 5 stars   \n",
       "\n",
       "   Product_Price  \n",
       "1        ₹76,490  \n",
       "2      ₹1,29,990  \n",
       "3        ₹77,999  \n",
       "4        ₹86,900  \n",
       "5        ₹57,490  \n",
       "6        ₹89,990  \n",
       "7        ₹92,400  \n",
       "8        ₹79,990  \n",
       "9        ₹85,890  \n",
       "10     ₹1,69,990  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20745712",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. This task will be done in following steps:\n",
    "\n",
    "First get the webpage https://www.ambitionbox.com/\n",
    "Click on the Job option as shown in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b01b00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:11: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_elements_by_xpath('//*[@id=\"headerWrapper\"]/nav/nav/a[6]')[0].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:14: DeprecationWarning: find_element_by_name is deprecated. Please use find_element(by=By.NAME, value=name) instead\n",
      "  driver.find_element_by_name('ab_jobsSearch').send_keys('Data Scientist')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:17: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_element_by_class_name('ctas-btn-medium').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:20: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('icon-chevron-right')[1].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:23: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('location_Noida').click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:26: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('text-center')[0].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:29: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  other = driver.find_elements_by_class_name('other-info')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:38: DeprecationWarning: find_elements_by_css_selector is deprecated. Please use find_elements(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  for i in driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:39: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Job_Role.append(i.find_elements_by_tag_name('h2')[0].text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:43: DeprecationWarning: find_elements_by_css_selector is deprecated. Please use find_elements(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  for i in driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]'):\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:44: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Company.append(i.find_elements_by_tag_name('p')[0].text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:48: DeprecationWarning: find_elements_by_css_selector is deprecated. Please use find_elements(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  a = driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:51: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  h = a[2].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:53: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  c = len(a[1].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p'))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:58: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  b.append(len(i.find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:61: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Salary.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[1].text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:68: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  b.append(len(i.find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:71: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Location.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[2].text)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/103955604.py:73: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  Location.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[1].text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_role</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist- AI/ML- R&amp;D</td>\n",
       "      <td>Chennai, Pune, Delhi NCR +4 more</td>\n",
       "      <td>₹ 6.5-16.5 LPA</td>\n",
       "      <td>EXL Services.com ( I ) Pvt. Ltd.</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bengaluru/Bangalore, Hyderabad/Secunderabad, G...</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oracle HCM BI Technical Lead/Manager (People A...</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>TECHNIP GLOBAL BUSINESS SERVICES PRIVATE LIMITED</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiring For Data Scientist + Python/R+ Predicti...</td>\n",
       "      <td>Gurgaon/Gurugram, Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>9-14 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Immediate Joiners</td>\n",
       "      <td>Pune, Mumbai, Bengaluru/Bangalore +1 more</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Bristlecone India Limited</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Machine Learning (5-14 yrs)</td>\n",
       "      <td>Bengaluru/Bangalore, Noida</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Zyoin</td>\n",
       "      <td>5-14 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bengaluru/Bangalore, Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist- Fresher Opening - Newgen Softw...</td>\n",
       "      <td>Noida</td>\n",
       "      <td>₹ 3-5 LPA</td>\n",
       "      <td>Newgen Software Technologies Ltd.</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Not Disclosed</td>\n",
       "      <td>Pitney Bowes India Pvt Ltd</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job_role  \\\n",
       "0                                     Data Scientist   \n",
       "1                         Data Scientist- AI/ML- R&D   \n",
       "2                                     Data Scientist   \n",
       "3  Oracle HCM BI Technical Lead/Manager (People A...   \n",
       "4  Hiring For Data Scientist + Python/R+ Predicti...   \n",
       "5                 Data Scientist - Immediate Joiners   \n",
       "6       Data Scientist - Machine Learning (5-14 yrs)   \n",
       "7                                     Data Scientist   \n",
       "8  Data Scientist- Fresher Opening - Newgen Softw...   \n",
       "9                              Senior Data Scientist   \n",
       "\n",
       "                                            Location          Salary  \\\n",
       "0                                              Noida   Not Disclosed   \n",
       "1                   Chennai, Pune, Delhi NCR +4 more  ₹ 6.5-16.5 LPA   \n",
       "2  Bengaluru/Bangalore, Hyderabad/Secunderabad, G...   Not Disclosed   \n",
       "3                                              Noida   Not Disclosed   \n",
       "4                            Gurgaon/Gurugram, Noida   Not Disclosed   \n",
       "5          Pune, Mumbai, Bengaluru/Bangalore +1 more   Not Disclosed   \n",
       "6                         Bengaluru/Bangalore, Noida   Not Available   \n",
       "7                         Bengaluru/Bangalore, Noida   Not Disclosed   \n",
       "8                                              Noida       ₹ 3-5 LPA   \n",
       "9                                              Noida   Not Disclosed   \n",
       "\n",
       "                                            Company Experience  \n",
       "0          Ericsson India Global Services Pvt. Ltd.    5-8 Yrs  \n",
       "1                  EXL Services.com ( I ) Pvt. Ltd.    2-6 Yrs  \n",
       "2                     GENPACT India Private Limited   7-12 Yrs  \n",
       "3  TECHNIP GLOBAL BUSINESS SERVICES PRIVATE LIMITED   7-12 Yrs  \n",
       "4                     GENPACT India Private Limited   9-14 Yrs  \n",
       "5                         Bristlecone India Limited    5-8 Yrs  \n",
       "6                                             Zyoin   5-14 Yrs  \n",
       "7                Ashkom Media India Private Limited    3-6 Yrs  \n",
       "8                 Newgen Software Technologies Ltd.    0-2 Yrs  \n",
       "9                        Pitney Bowes India Pvt Ltd   6-10 Yrs  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium webdriver & time\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#extract data from url\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "time.sleep(10)\n",
    "#click on jobs\n",
    "driver.find_elements_by_xpath('//*[@id=\"headerWrapper\"]/nav/nav/a[6]')[0].click()\n",
    "time.sleep(5)\n",
    "#Enter Data Scientist on the seatch bar\n",
    "driver.find_element_by_name('ab_jobsSearch').send_keys('Data Scientist')\n",
    "time.sleep(5)\n",
    "#click on the seatch button\n",
    "driver.find_element_by_class_name('ctas-btn-medium').click()\n",
    "time.sleep(5)\n",
    "#click search button on the location\n",
    "driver.find_elements_by_class_name('icon-chevron-right')[1].click()\n",
    "time.sleep(5)\n",
    "#Select noida location\n",
    "driver.find_element_by_id('location_Noida').click()\n",
    "time.sleep(5)\n",
    "#click on load more jobs\n",
    "driver.find_elements_by_class_name('text-center')[0].click()\n",
    "time.sleep(5)\n",
    "#find the job text\n",
    "other = driver.find_elements_by_class_name('other-info')\n",
    "\n",
    "#Extract job Experience\n",
    "Experience=[]\n",
    "for i in range(len(other)):\n",
    "    Experience.append(other[i].text.split('\\n')[0])\n",
    "\n",
    "#Extract job role\n",
    "Job_Role=[]\n",
    "for i in driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]'):\n",
    "    Job_Role.append(i.find_elements_by_tag_name('h2')[0].text)\n",
    "    \n",
    "#Extract job Company\n",
    "Company=[]\n",
    "for i in driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]'):\n",
    "    Company.append(i.find_elements_by_tag_name('p')[0].text)\n",
    "    \n",
    "#job ratings varies for each jpb post,counting the no of attributes in each post\n",
    "\n",
    "a = driver.find_elements_by_css_selector('div[itemprop=\"itemListElement\"]')\n",
    "\n",
    "#We find each post has around 4 attributes with ratings\n",
    "h = a[2].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')\n",
    "\n",
    "c = len(a[1].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p'))\n",
    "\n",
    "#Using exception extract salary info\n",
    "b,Salary=[],[]\n",
    "for i in a:\n",
    "    b.append(len(i.find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')))\n",
    "for i in range(20):\n",
    "    if b[i]==c:\n",
    "        Salary.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[1].text)\n",
    "    else:\n",
    "        Salary.append('Not Available')\n",
    "        \n",
    "#Using exception extract location info\n",
    "b,Location=[],[]\n",
    "for i in a:\n",
    "    b.append(len(i.find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')))\n",
    "for i in range(20):\n",
    "    if b[i]==4:\n",
    "        Location.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[2].text)\n",
    "    else:\n",
    "        Location.append(a[i].find_elements_by_class_name('other-info')[0].find_elements_by_tag_name('p')[1].text)\n",
    "        \n",
    "#Extract dataframe by importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "df_9 = pd.DataFrame(zip(Job_Role,Location,Salary,Company,Experience),columns=('Job_role','Location','Salary','Company','Experience'))\n",
    "\n",
    "#extracting data for 10 posts\n",
    "df_9.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc09ce",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation. You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary. The above task will be, done as shown in the below steps:\n",
    "\n",
    "First get the webpage https://www.ambitionbox.com/\n",
    "Click on the salaries option as shown in the image.\n",
    "After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”. You have to scrape the data ticked in the above image.\n",
    "Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "Store the data in a dataframe. Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "071ea171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2570262249.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wb.Chrome('chromedriver.exe')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2570262249.py:11: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  driver.find_elements_by_tag_name('a')[4].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2570262249.py:14: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('jobProfileSearchbox').send_keys('Data Scientist')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2570262249.py:17: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  driver.find_elements_by_class_name('icon-search')[0].click()\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2570262249.py:20: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  a = driver.find_elements_by_class_name('company-info-wrapper')\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2570262249.py:23: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  r = len(driver.find_elements_by_class_name('company-info-wrapper'))\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp/ipykernel_12068/2570262249.py:34: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  s = driver.find_elements_by_class_name('salary-range-wrapper')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>No_of_Salaries</th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Lowest_Salary</th>\n",
       "      <th>Average_Salary</th>\n",
       "      <th>Highest_Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>based on 21 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 10.4L</td>\n",
       "      <td>₹ 33.1L</td>\n",
       "      <td>₹ 97.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arista Networks</td>\n",
       "      <td>based on 13 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 10.7L</td>\n",
       "      <td>₹ 24.7L</td>\n",
       "      <td>₹ 38.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Microsoft Corporation</td>\n",
       "      <td>based on 255 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 13.0L</td>\n",
       "      <td>₹ 23.7L</td>\n",
       "      <td>₹ 45.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>based on 17 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 12.0L</td>\n",
       "      <td>₹ 22.7L</td>\n",
       "      <td>₹ 34.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tekion</td>\n",
       "      <td>based on 33 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 12.0L</td>\n",
       "      <td>₹ 21.0L</td>\n",
       "      <td>₹ 33.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>based on 108 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 8.0L</td>\n",
       "      <td>₹ 20.6L</td>\n",
       "      <td>₹ 45.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flipkart</td>\n",
       "      <td>based on 62 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 7.5L</td>\n",
       "      <td>₹ 20.4L</td>\n",
       "      <td>₹ 31.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PayPal</td>\n",
       "      <td>based on 27 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 12.0L</td>\n",
       "      <td>₹ 20.2L</td>\n",
       "      <td>₹ 31.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Servicenow Software Development India</td>\n",
       "      <td>based on 54 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 13.0L</td>\n",
       "      <td>₹ 20.2L</td>\n",
       "      <td>₹ 26.3L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>based on 82 salaries</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 19.8L</td>\n",
       "      <td>₹ 32.5L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Company         No_of_Salaries  \\\n",
       "0                                 Google   based on 21 salaries   \n",
       "1                        Arista Networks   based on 13 salaries   \n",
       "2                  Microsoft Corporation  based on 255 salaries   \n",
       "3                          Goldman Sachs   based on 17 salaries   \n",
       "4                                 Tekion   based on 33 salaries   \n",
       "5                                 Amazon  based on 108 salaries   \n",
       "6                               Flipkart   based on 62 salaries   \n",
       "7                                 PayPal   based on 27 salaries   \n",
       "8  Servicenow Software Development India   based on 54 salaries   \n",
       "9                                Walmart   based on 82 salaries   \n",
       "\n",
       "            Job_Role Lowest_Salary Average_Salary Highest_Salary  \n",
       "0  Software Engineer       ₹ 10.4L        ₹ 33.1L        ₹ 97.0L  \n",
       "1  Software Engineer       ₹ 10.7L        ₹ 24.7L        ₹ 38.0L  \n",
       "2  Software Engineer       ₹ 13.0L        ₹ 23.7L        ₹ 45.0L  \n",
       "3  Software Engineer       ₹ 12.0L        ₹ 22.7L        ₹ 34.0L  \n",
       "4  Software Engineer       ₹ 12.0L        ₹ 21.0L        ₹ 33.0L  \n",
       "5  Software Engineer        ₹ 8.0L        ₹ 20.6L        ₹ 45.0L  \n",
       "6  Software Engineer        ₹ 7.5L        ₹ 20.4L        ₹ 31.0L  \n",
       "7  Software Engineer       ₹ 12.0L        ₹ 20.2L        ₹ 31.0L  \n",
       "8  Software Engineer       ₹ 13.0L        ₹ 20.2L        ₹ 26.3L  \n",
       "9  Software Engineer       ₹ 11.0L        ₹ 19.8L        ₹ 32.5L  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import selenium, webdriver time\n",
    "from selenium import webdriver as wb\n",
    "import time\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "#extract url\n",
    "driver = wb.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "time.sleep(3)\n",
    "# Click on the salaries option\n",
    "driver.find_elements_by_tag_name('a')[4].click()\n",
    "time.sleep(3)\n",
    "#After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist”  \n",
    "driver.find_element_by_id('jobProfileSearchbox').send_keys('Data Scientist')\n",
    "time.sleep(3)\n",
    "#then click on “Data Scientist\n",
    "driver.find_elements_by_class_name('icon-search')[0].click()\n",
    "time.sleep(3)\n",
    "#Extract job profile data\n",
    "a = driver.find_elements_by_class_name('company-info-wrapper')\n",
    "time.sleep(3)\n",
    "#Calculate the length of product data,some dont have review rating\n",
    "r = len(driver.find_elements_by_class_name('company-info-wrapper'))\n",
    "\n",
    "#now split for all posts\n",
    "b = []\n",
    "for i in range(r):\n",
    "    b.append(a[i].text.split('\\n'))\n",
    "    \n",
    "#map using zip option\n",
    "Company,No_of_Salaries,Job_Role,Dot,Experience = list(zip(*b))\n",
    "\n",
    "#Extract salary info\n",
    "s = driver.find_elements_by_class_name('salary-range-wrapper')\n",
    "\n",
    "#Extract ranges of salary\n",
    "d = []\n",
    "for i in range(10):\n",
    "    d.append(s[i].text.split('\\n'))\n",
    "    \n",
    "#map using zip option\n",
    "Average_Salary,Lowest_Salary,Highest_Salary = list(zip(*d))\n",
    "\n",
    "#Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Extract datafame\n",
    "df_10 = pd.DataFrame(zip(Company,No_of_Salaries,Job_Role,Lowest_Salary,Average_Salary,Highest_Salary),columns=('Company','No_of_Salaries','Job_Role','Lowest_Salary','Average_Salary','Highest_Salary'))\n",
    "\n",
    "#data for the first 10 posts\n",
    "df_10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c030512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
